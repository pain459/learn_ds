import torch
from torch.utils.data import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling

# Define a custom Dataset class
class TextDataset(Dataset):
    def __init__(self, tokenized_text):
        self.input_ids = tokenized_text['input_ids']
        self.attention_mask = tokenized_text['attention_mask']
    
    def __len__(self):
        return len(self.input_ids)
    
    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx]
        }

# Load the LLaMA model and tokenizer
model_name = "meta-llama/Llama-3.2-1B"  # Update to the exact model you're using
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set the padding token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token

# Define training arguments
training_args = TrainingArguments(
    output_dir="./output",
    per_device_train_batch_size=1,
    num_train_epochs=3,
    save_steps=10,
    logging_dir="./logs",
    overwrite_output_dir=True  # Overwrite the output directory if it exists
)

# Fine-tune LLaMA on our document
def fine_tune_llama(tokens):
    # Convert tokens to a dataset
    dataset = TextDataset(tokens)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator
    )
    trainer.train()
    
    # Get the latest checkpoint directory
    latest_checkpoint_dir = "./output/checkpoint-3"
    
    # Explicitly save the tokenizer in the checkpoint directory
    tokenizer.save_pretrained(latest_checkpoint_dir)

# Example usage
if __name__ == "__main__":
    # Assume preprocess_document function is available from preprocess.py
    from preprocess import preprocess_document
    tokens = preprocess_document("wikipedia", "Machine_learning")
    
    # Check if tokens were fetched successfully
    if tokens:
        fine_tune_llama(tokens)
