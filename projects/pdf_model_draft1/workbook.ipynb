{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a636ef25",
   "metadata": {},
   "source": [
    "PDF → Text → Chunking → Embedding → Indexing → Q&A → Model Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10ddbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "import pymupdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "text = extract_text_from_pdf(\"the-illusion-of-thinking.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af52f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text\n",
    "\n",
    "def chunk_text(text, chunk_size = 500000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "chunks = chunk_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c3a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravik/src_git/learn_ds/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
    "\n",
    "# IMPORTANT: e5 models expect \"passage: ...\" or \"query: ...\" prefixes\n",
    "embeddings = model.encode([\"passage: \" + chunk for chunk in chunks], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0cbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf272e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Determine dimensionality from the first vector\n",
    "dimension = embeddings[0].shape[0]\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Convert to correct format\n",
    "embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save index and chunks\n",
    "faiss.write_index(index, \"faiss.index\")\n",
    "with open(\"doc_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b401e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questioning Answering over Retrieved Context\n",
    "import numpy as np\n",
    "\n",
    "query_embedding = model.encode(\"query: What is the architecture described?\")\n",
    "query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_embedding, k=1)  # D = distances, I = indices\n",
    "context = chunks[I[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e21017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Context:\n",
      " The Illusion of Thinking:\n",
      "Understanding the Strengths and Limitations of Reasoning Models\n",
      "via the Lens of Problem Complexity\n",
      "Parshin Shojaee∗†\n",
      "Iman Mirzadeh∗\n",
      "Keivan Alizadeh\n",
      "Maxwell Horton\n",
      "Samy Bengio\n",
      "Mehrdad Farajtabar\n",
      "Apple\n",
      "Abstract\n",
      "Recent generations of frontier language models have introduced Large Reasoning Models\n",
      "(LRMs) that generate detailed thinking processes before providing answers. While these models\n",
      "demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal-\n",
      "ing properties, and limitations remain insufficiently understood. Current evaluations primarily fo-\n",
      "cus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How-\n",
      "ever, this evaluation paradigm often suffers from data contamination and does not provide insights\n",
      "into the reasoning traces’ structure and quality. In this work, we systematically investigate these\n",
      "gaps with the help of controllable puzzle environments that allow precise manipulation of composi-\n",
      "tional complexity while maintaining consistent logical structures. This setup enables the analysis\n",
      "of not only final answers but also the internal reasoning traces, offering insights into how LRMs\n",
      "“think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs\n",
      "face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-\n",
      "intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then\n",
      "declines despite having an adequate token budget. By comparing LRMs with their standard LLM\n",
      "counterparts under equivalent inference compute, we identify three performance regimes: (1) low-\n",
      "complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity\n",
      "tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks\n",
      "where both models experience complete collapse. We found that LRMs have limitations in exact\n",
      "computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We\n",
      "also investigate the reasoning traces in more depth, studying the patterns of explored solutions\n",
      "and analyzing the models’ computational behavior, shedding light on their strengths, limitations,\n",
      "and ultimately raising crucial questions about their true reasoning capabilities.\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have recently evolved to include specialized variants explicitly\n",
      "designed for reasoning tasks—Large Reasoning Models (LRMs) such as OpenAI’s o1/o3 [1, 2],\n",
      "DeepSeek-R1 [3], Claude 3.7 Sonnet Thinking [4], and Gemini Thinking [5]. These models are new\n",
      "artifacts, characterized by their “thinking” mechanisms such as long Chain-of-Thought (CoT) with\n",
      "self-reflection, and have demonstrated promising results across various reasoning benchmarks. Their\n",
      "∗Equal contribution.\n",
      "†Work done during an internship at Apple.\n",
      "{p_shojaee, imirzadeh, kalizadehvahid, mchorton, bengio, farajtabar}@apple.com\n",
      "1\n",
      "\n",
      "Initial State\n",
      "Middle State\n",
      "1\n",
      "Peg 0\n",
      "[1, 0, 2]\n",
      "[2, 0, 1]\n",
      "[1, 2, 1]\n",
      "[3, 0, 2]\n",
      "[1, 1, 0]\n",
      "[2, 1, 2]\n",
      "[1, 0, 2]\n",
      "Peg 1\n",
      "Peg 2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "Target State\n",
      "<think>\n",
      "Move disk 1 from peg 0 to peg 2 ...  \n",
      "moves = [\n",
      "]\n",
      "Let me double-check this...\n",
      "</think>\n",
      "  [1, 0, 2],\n",
      "  [2, 0, 1],\n",
      "  [1, 2, 1],\n",
      "  [3, 0, 2],\n",
      "  [1, 1, 0],\n",
      "  [2, 1, 2],\n",
      "  [1, 0, 2],\n",
      "<answer> the final answer is moves=... \n",
      "</answer>\n",
      "LLM Response\n",
      "extract moves  from thoughts \n",
      "(for analysis)\n",
      "extract final answer \n",
      "(for measuring accuracy)\n",
      "1 2 3 4 5 6 7 8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (number of disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Claude 3.7\n",
      "(+thinking)\n",
      "Claude 3.7\n",
      "1 2 3 4 5 6 7 8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (number of disks)\n",
      "0\n",
      "5,000\n",
      "10,000\n",
      "15,000\n",
      "20,000\n",
      "Response Length (Tokens)\n",
      "Claude 3.7\n",
      "(+thinking)\n",
      "Claude 3.7\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "Complexity (number of disks)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Position within Thoughts\n",
      "Correct Solutions\n",
      "Incorrect Solutions\n",
      "Figure 1: Top: Our setup enables verification of both final answers and intermediate reasoning traces,\n",
      "allowing detailed analysis of model thinking behavior. Bottom left & middle: At low complexity,\n",
      "non-thinking models are more accurate and token-efficient. As complexity increases, reasoning models\n",
      "outperform but require more tokens—until both collapse beyond a critical threshold, with shorter\n",
      "traces.\n",
      "Bottom right: For correctly solved cases, Claude 3.7 Thinking tends to find answers early\n",
      "at low complexity and later at higher complexity. In failed cases, it often fixates on an early wrong\n",
      "answer, wasting the remaining token budget. Both cases reveal inefficiencies in the reasoning process.\n",
      "emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning\n",
      "and problem-solving tasks, with some researchers proposing them as significant steps toward more\n",
      "general artificial intelligence capabilities.\n",
      "Despite these claims and performance advancements, the fundamental benefits and limitations of\n",
      "LRMs remain insufficiently understood. Critical questions still persist: Are these models capable\n",
      "of generalizable reasoning, or are they leveraging different forms of pattern matching [6]? How\n",
      "does their performance scale with increasing problem complexity? How do they compare to their\n",
      "non-thinking standard LLM counterparts when provided with the same inference token compute?\n",
      "Most importantly, what are the inherent limitations of current reasoning approaches, and what\n",
      "improvements might be necessary to advance toward more robust reasoning capabilities?\n",
      "We believe the lack of systematic analyses investigating these questions is due to limitations in\n",
      "current evaluation paradigms. Existing evaluations predominantly focus on established mathematical\n",
      "and coding benchmarks, which, while valuable, often suffer from data contamination issues and do\n",
      "not allow for controlled experimental conditions across different settings and complexities. Moreover,\n",
      "these evaluations do not provide insights into the structure and quality of reasoning traces. To\n",
      "understand the reasoning behavior of these models more rigorously, we need environments that\n",
      "enable controlled experimentation.\n",
      "In this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem\n",
      "2\n",
      "\n",
      "complexity. Rather than standard benchmarks (e.g., math problems), we adopt controllable puzzle en-\n",
      "vironments that let us vary complexity systematically—by adjusting puzzle elements while preserving\n",
      "the core logic—and inspect both solutions and internal reasoning (Fig. 1, top). These puzzles: (1) of-\n",
      "fer fine-grained control over complexity; (2) avoid contamination common in established benchmarks;\n",
      "(3) require only the explicitly provided rules, emphasizing algorithmic reasoning; and (4) support\n",
      "rigorous, simulator-based evaluation, enabling precise solution checks and detailed failure analyses.\n",
      "Our empirical investigation reveals several key findings about current Language Reasoning Models\n",
      "(LRMs): First, despite their sophisticated self-reflection mechanisms learned through reinforcement\n",
      "learning, these models fail to develop generalizable problem-solving capabilities for planning tasks,\n",
      "with performance collapsing to zero beyond a certain complexity threshold. Second, our comparison\n",
      "between LRMs and standard LLMs under equivalent inference compute reveals three distinct reason-\n",
      "ing regimes (Fig. 1, bottom). For simpler, low-compositional problems, standard LLMs demonstrate\n",
      "greater efficiency and accuracy. As problem complexity moderately increases, thinking models gain\n",
      "an advantage. However, when problems reach high complexity with longer compositional depth,\n",
      "both model types experience complete performance collapse (Fig. 1, bottom left). Notably, near\n",
      "this collapse point, LRMs begin reducing their reasoning effort (measured by inference-time tokens)\n",
      "as problem complexity increases, despite operating well below generation length limits (Fig. 1,\n",
      "bottom middle). This suggests a fundamental inference time scaling limitation in LRMs’ reasoning\n",
      "capabilities relative to problem complexity. Finally, our analysis of intermediate reasoning traces or\n",
      "thoughts reveals complexity-dependent patterns: In simpler problems, reasoning models often identify\n",
      "correct solutions early but inefficiently continue exploring incorrect alternatives—an “overthinking”\n",
      "phenomenon. At moderate complexity, correct solutions emerge only after extensive exploration\n",
      "of incorrect paths. Beyond a certain complexity threshold, models completely fail to find correct\n",
      "solutions (Fig. 1, bottom right). This indicates LRMs possess limited self-correction capabilities\n",
      "that, while valuable, reveal fundamental inefficiencies and clear scaling limitations.\n",
      "These findings highlight both the strengths and limitations of existing LRMs, raising questions\n",
      "about the nature of reasoning in these systems with important implications for their design and\n",
      "deployment. Our key contributions are:\n",
      "• We question the current evaluation paradigm of LRMs on established math benchmarks and\n",
      "design a controlled experimental testbed by leveraging algorithmic puzzle environments that enable\n",
      "controllable experimentation with respect to problem complexity.\n",
      "• We show that state-of-the-art LRMs (e.g., o3-mini, DeepSeek-R1, Claude-3.7-Sonnet-Thinking)\n",
      "still fail to develop generalizable problem-solving capabilities, with accuracy ultimately collapsing\n",
      "to zero beyond certain complexities across different environments.\n",
      "• We find that there exists a scaling limit in the LRMs’ reasoning effort with respect to problem\n",
      "complexity, evidenced by the counterintuitive decreasing trend in the thinking tokens after a\n",
      "complexity point.\n",
      "• We question the current evaluation paradigm based on final accuracy and extend our evaluation\n",
      "to intermediate solutions of thinking traces with the help of deterministic puzzle simulators. Our\n",
      "analysis reveals that as problem complexity increases, correct solutions systematically emerge at\n",
      "later positions in thinking compared to incorrect ones, providing quantitative insights into the\n",
      "self-correction mechanisms within LRMs.\n",
      "• We uncover surprising limitations in LRMs’ ability to perform exact computation, including their\n",
      "failure to benefit from explicit algorithms and their inconsistent reasoning across puzzle types.\n",
      "3\n",
      "\n",
      "2\n",
      "Related Works\n",
      "Reasoning in Language Models.\n",
      "Large Language Models (LLMs) undergo multiple costly\n",
      "training phases using vast amounts of training data. While these LLMs demonstrate promising\n",
      "language understanding with strong compression capabilities, their intelligence and reasoning abilities\n",
      "remain a critical topic of scientific debate [7, 8]. Earlier iterations of LLMs [9, 10, 11] exhibited\n",
      "poor performance on reasoning benchmarks [12, 13, 14, 6]. To address these shortcomings, several\n",
      "approaches have been explored with the common theme among them being “scaling” both the training\n",
      "data and test-time computation. For instance, generating a Chain of Thought (CoT) [15, 16, 17, 18]\n",
      "and incorporating self-verification [19, 20, 21] prior to the final answer have been shown to improve\n",
      "model performance. However, obtaining high-quality and scalable CoT data is quite expensive\n",
      "due to its scarcity. Another line of research focuses on compensating for the lack of supervised\n",
      "data by teaching models to think more effectively through supervised learning or reinforcement\n",
      "learning [22, 23, 24, 25, 26, 27]. A notable open-source example of these improvements is Deepseek-\n",
      "R1 [3], which demonstrated that applying RL with verifiable rewards can significantly enhance model\n",
      "performance, matching that of closed models like OpenAI’s o1 [2], leading to a new generation of\n",
      "language models referred to as Large Reasoning Models (LRMs) such as Gemini flash thinking [5],\n",
      "Claude 3.7 Sonnet thinking [4], etc.\n",
      "Understanding Large Reasoning Models.\n",
      "Recent studies have explored various aspects of\n",
      "reasoning behavior: Large Reasoning Models have shown emergent behaviors such as discrepancy\n",
      "between thought traces and final answers [28, 29] as well as efficiency concerns through what\n",
      "researchers term the “overthinking phenomenon” [30, 31, 32, 33], where models produce verbose,\n",
      "redundant outputs, even after finding the solution, creating significant inference computational\n",
      "overhead. In this work, we systematically analyze how much model thinks w.r.t task complexity.\n",
      "Recently, Ballon et al. [34] demonstrated that in newer LRMs accuracy generally declines when\n",
      "thinking increases in math problems, in contrast we observe when in controlled puzzle environment\n",
      "difficulty passes a certain level the model starts to think less and opposite corelation of thinking and\n",
      "task complexity only happens up to some threshold. Yue et al. [35] questioned whether reinforcement\n",
      "learning truly elicits novel reasoning patterns and shows pass@k of reasoning vs non-reasoning models\n",
      "converge to the same point. We also observe that in MATH-500 pass@k is close for reasoning versus\n",
      "non-reasoning models but we observed different patterns under medium and high complexity of\n",
      "puzzles, which is not easily observable on established math benchmarks used in common evaluations.\n",
      "Controllable Evaluation Environments.\n",
      "Unlike earlier studies that focused on mathematical\n",
      "problems to evaluate the reasoning capabilities of language models, this work introduces controllable\n",
      "puzzle environments. These environments allow for precise manipulation of problem complexity while\n",
      "maintaining consistent logical processes, enabling a more rigorous analysis of reasoning patterns and\n",
      "limitations. Controllable environments are not uncommon in the literature [12, 36, 37]. However,\n",
      "our primary aim is not to propose a new benchmark; instead, we use these benchmarks as tools\n",
      "for designing experiments to understand the reasoning capabilities of language models. A closely\n",
      "related study by Valmeekam et al. [38] demonstrated that o1-models show significant performance\n",
      "improvements compared to previous models. Our work offers additional insights, such as examining\n",
      "pairs of thinking/non-thinking models (e.g., DeepSeek-R1/V3, Claude 3.7 Sonnet thinking/non-\n",
      "thinking). Furthermore, we study the reasoning traces of the LRMs in more depth, revealing different\n",
      "behaviors across various complexity levels.\n",
      "Overall, the promising results from recent LRMs raise a critical question: how much have the\n",
      "previously reported limitations of LLMs been improved? In this work, we move beyond merely\n",
      "measuring the performance of these LRMs. We analyze how well these LRMs tackle problems of\n",
      "varying complexities and examine the properties of their reasoning processes.\n",
      "4\n",
      "\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "Inference Compute Budget (Tokens)\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "pass@k\n",
      "MATH-500\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME24\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME25\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "Inference Compute Budget (Tokens)\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "pass@k\n",
      "MATH-500\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME24\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME25\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "Figure 2: Comparative analysis of thinking versus non-thinking models across math benchmarks\n",
      "reveals inconsistent performance patterns. While results on the MATH-500 dataset show comparable\n",
      "performance between both model types, the thinking models demonstrate superior performance\n",
      "on AIME24 and AIME25 benchmarks. Additionally, the observed performance degradation from\n",
      "AIME24 to AIME25 highlights the vulnerability of these benchmarks to data contamination issues.\n",
      "3\n",
      "Math and Puzzle Environments\n",
      "Currently, it is not clear whether the performance enhancements observed in recent RL-based\n",
      "thinking models are attributable to increased exposure to established mathematical benchmark\n",
      "data, to the significantly greater inference compute allocated to thinking tokens, or to reasoning\n",
      "capabilities developed by RL-based training? Recent studies [35, 39] have explored this question\n",
      "with established math benchmarks by comparing the upper-bound capabilities (pass@k) of RL-based\n",
      "thinking models with their non-thinking standard LLM counterparts. They have shown that under\n",
      "equivalent inference token budgets, non-thinking LLMs can eventually reach performance comparable\n",
      "to thinking models on benchmarks like MATH500 [40] and AIME24 [41]. We also conducted our\n",
      "comparative analysis of frontier LRMs like Claude-3.7-Sonnet (with vs. without thinking) and\n",
      "DeepSeek (R1 vs. V3). Our results (shown in Fig. 2) confirm that, on the MATH500 dataset, the\n",
      "pass@k performance of thinking models is comparable to their non-thinking counterparts when\n",
      "provided with the same inference token budget. However, we observed that this performance gap\n",
      "widens on the AIME24 benchmark and widens further on AIME25. This widening gap presents\n",
      "an interpretive challenge. It could be attributed to either: (1) increasing complexity requiring\n",
      "more sophisticated reasoning processes, thus revealing genuine advantages of the thinking models\n",
      "for more complex problems, or (2) reduced data contamination in newer benchmarks (particularly\n",
      "AIME25). Interestingly, human performance on AIME25 was actually higher than on AIME24\n",
      "[42, 43], suggesting that AIME25 might be less complex. Yet models perform worse on AIME25\n",
      "than AIME24—potentially suggesting data contamination during the training of frontier LRMs.\n",
      "Given these non-justified observations and the fact that mathematical benchmarks do not allow for\n",
      "controlled manipulation of problem complexity, we turned to puzzle environments that enable more\n",
      "precise and systematic experimentation.\n",
      "5\n",
      "\n",
      "Initial State\n",
      "Middle State\n",
      "moves\n",
      "moves\n",
      "Tower of Hanoi\n",
      "Checkers Jumping\n",
      "River Crossing\n",
      "Blocks World\n",
      "Target State\n",
      "Figure 3: Illustration of the four puzzle environments. Columns show the progression from initial\n",
      "state (top) through intermediate state (middle) to target state (bottom) for puzzles: Tower\n",
      "of Hanoi (disk transfer across pegs), Checkers Jumping (position swapping of colored tokens), River\n",
      "Crossing (transporting entities across a river), and Blocks World (stack reconfiguration).\n",
      "3.1\n",
      "Puzzle Environments\n",
      "We evaluate LRM reasoning on four controllable puzzles spanning compositional depth, planning\n",
      "complexity, and distributional settings. The puzzles are defined below and illustrated in Fig. 3.\n",
      "Tower of Hanoi is a puzzle featuring three pegs and n disks of different sizes stacked on the first\n",
      "peg in size order (largest at bottom). The goal is to transfer all disks from the first peg to the third\n",
      "peg. Valid moves include moving only one disk at a time, taking only the top disk from a peg, and\n",
      "never placing a larger disk on top of a smaller one. The difficulty in this task can be controlled by\n",
      "the number of initial disks as the minimum number of required moves with n initial disks will be\n",
      "2n −1. However, in this work we do not grade for optimality of final solution and only measuring\n",
      "the correctness of each move and reaching the target state.\n",
      "Checker Jumping is a one-dimensional puzzle arranging red checkers, blue checkers, and a single\n",
      "empty space in a line. The objective is to swap the positions of all red and blue checkers, effectively\n",
      "mirroring the initial configuration. Valid moves include sliding a checker into an adjacent empty\n",
      "space or jumping over exactly one checker of the opposite color to land in an empty space. No checker\n",
      "can move backward in the puzzle process. The complexity of this task can be controlled by the\n",
      "number of checkers: with 2n checkers, the minimum number of moves required will be (n + 1)2 −1.\n",
      "River Crossing is a constraint satisfaction planning puzzle involving n actors and their corresponding\n",
      "n agents who must cross a river using a boat. The goal is to transport all 2n individuals from the\n",
      "left bank to the right bank. The boat can carry at most k individuals and cannot travel empty.\n",
      "Invalid situations arise when an actor is in the presence of another agent without their own agent\n",
      "present, as each agent must protect their client from competing agents. The complexity of this task\n",
      "can also be controlled by the number of actor/agent pairs present. For n = 2, n = 3 pairs, we use\n",
      "boat capacity of k = 2 and for larger number of pairs we use k = 3.\n",
      "Blocks World is a block-stacking puzzle requiring rearrangement of blocks from an initial configu-\n",
      "ration into a specified goal configuration. The objective is to find the minimum number of moves\n",
      "needed for this transformation. Valid moves are restricted to the topmost block of any stack, which\n",
      "can be placed either on an empty stack or on top of another block. The complexity in this task can\n",
      "be controlled by the number of blocks present.\n",
      "6\n",
      "\n",
      "Figure 4: Accuracy of thinking models (Claude 3.7 Sonnet with thinking, DeepSeek-R1) versus their\n",
      "non-thinking counterparts (Claude 3.7 Sonnet, DeepSeek-V3) across all puzzle environments and\n",
      "varying levels of problem complexity.\n",
      "4\n",
      "Experiments & Results\n",
      "4.1\n",
      "Experimental Setup\n",
      "Most of our experiments are conducted on reasoning models and their non-thinking counterparts,\n",
      "such as Claude 3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3. We chose these models\n",
      "because they allow access to the thinking tokens, unlike models such as OpenAI’s o-series. For\n",
      "experiments focused solely on final accuracy, we also report results on the o-series models. For Claude\n",
      "3.7 Sonnet models, we allow the maximum token budget (64k). Similarly, for DeepSeek-R1/V3\n",
      "models on local servers, we allow the maximum length to be up to 64k tokens. For each puzzle\n",
      "instance, we generate 25 samples and report the average performance of each model across them.\n",
      "Comprehensive details of our experimental setup and results are provided in the Appendix.\n",
      "4.2\n",
      "How Does Complexity Affect Reasoning?\n",
      "4.2.1\n",
      "Three Regimes of Complexity\n",
      "Motivated by the observations in Fig. 2, to systematically investigate the impact of problem complexity\n",
      "on reasoning behavior, we conducted experiments comparing thinking and non-thinking model\n",
      "pairs across our controlled puzzle environments. Our analysis focused on matched pairs of LLMs\n",
      "with identical model backbones, specifically Claude-3.7-Sonnet (w. vs. w/o thinking) and DeepSeek\n",
      "(R1 vs. V3). In each puzzle, we vary the complexity by manipulating problem size N (representing\n",
      "disk count, checker count, block count, or crossing elements).\n",
      "Fig. 4 presents the accuracy of both model types as a function of problem complexity across all\n",
      "puzzle environments. Complementing this, Fig. 5 shows the upper bound performance capabilities\n",
      "(pass@k) of these model pairs under equivalent inference token compute (averaged across all puzzles),\n",
      "extending earlier analyses from mathematical benchmarks (Fig. 2) to the controlled puzzle environ-\n",
      "ments. Results from both these figures demonstrate that, unlike observations from math, there exists\n",
      "three regimes in the behavior of these models with respect to complexity. In the first regime where\n",
      "problem complexity is low, we observe that non-thinking models are capable to obtain performance\n",
      "comparable to, or even better than thinking models with more token-efficient inference. In the\n",
      "7\n",
      "\n",
      "Figure 5: Pass@k performance of thinking vs. non-thinking models across equivalent compute\n",
      "budgets in puzzle environments of low , medium , and high complexity. Non-thinking models excel\n",
      "in simple problems, thinking models show advantages at medium complexity, while both approaches\n",
      "fail at high complexity regardless of compute allocation.\n",
      "second regime with medium complexity, the advantage of reasoning models capable of generating\n",
      "long chain-of-thought begin to manifest, and the performance gap between model pairs increases. The\n",
      "most interesting regime is the third regime where problem complexity is higher and the performance\n",
      "of both models have collapsed to zero. Results show that while thinking models delay this collapse,\n",
      "they also ultimately encounter the same fundamental limitations as their non-thinking counterparts.\n",
      "4.2.2\n",
      "Collapse of Reasoning Models\n",
      "We next examine how different specialized reasoning models equipped with thinking tokens respond\n",
      "to increasing problem complexity. Our experiments evaluate five state-of-the-art thinking models:\n",
      "o3-mini (medium and high configurations), DeepSeek-R1, DeepSeek-R1-Qwen-32B, and Claude-3.7-\n",
      "Sonnet (thinking). Fig. 6 demonstrates these models’ performance in terms of accuracy (top) and\n",
      "thinking token usage (bottom) across varying complexity levels. Results show that all reasoning\n",
      "models exhibit a similar pattern with respect to complexity: accuracy progressively declines as\n",
      "problem complexity increases until reaching complete collapse (zero accuracy) beyond a model-\n",
      "specific complexity threshold. Analysis of inference thinking token compute also reveals an intriguing\n",
      "pattern in thinking token allocation learned by these models. We observe that reasoning models\n",
      "initially increase their thinking tokens proportionally with problem complexity. However, upon\n",
      "approaching a critical threshold—which closely corresponds to their accuracy collapse point—models\n",
      "counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty. This\n",
      "phenomenon is most pronounced in o3-mini variants and less severe in the Claude-3.7-Sonnet\n",
      "(thinking) model. Notably, despite operating well below their generation length limits with ample\n",
      "inference budget available, these models fail to take advantage of additional inference compute during\n",
      "the thinking phase as problems become more complex. This behavior suggests a fundamental scaling\n",
      "limitation in the thinking capabilities of current reasoning models relative to problem complexity.\n",
      "8\n",
      "\n",
      "Figure 6: Accuracy and thinking tokens vs. problem complexity for reasoning models across puzzle\n",
      "environments. As complexity increases, reasoning models initially spend more tokens while accuracy\n",
      "declines gradually, until a critical point where reasoning collapses—performance drops sharply and\n",
      "reasoning effort decreases.\n",
      "4.3\n",
      "What Happens Inside the Thoughts of Reasoning Models?\n",
      "To gain deeper insights into the thinking processes of reasoning models, we conducted a fine-grained\n",
      "analysis of their reasoning traces. As shown in Fig. 1, our setup with puzzle environments allows us\n",
      "to look beyond final answer and obtain more detailed insight into the reasoning traces (“thoughts”)\n",
      "produced by these models. We extract and analyze the intermediate solutions explored within the\n",
      "thoughts of a model with the help of puzzle simulators. Our investigation examines the patterns and\n",
      "characteristics of these intermediate solutions, their correctness relative to their sequential position\n",
      "in the reasoning process, and how these patterns evolve with increasing problem complexity. For\n",
      "this analysis, we focus on the reasoning traces generated by Claude-3.7-Sonnet-Thinking across\n",
      "our puzzle suite. For each intermediate solution identified within the traces, we recorded: (1) its\n",
      "relative position within the reasoning trace (normalized by total thought length), (2) its correctness\n",
      "as validated by our puzzle simulators, and (3) the complexity of the corresponding problem. This\n",
      "allows to characterize the progression and accuracy of solution development throughout the reasoning\n",
      "process.\n",
      "Fig. 7a demonstrates the relation between the position of intermediate solutions within thoughts, their\n",
      "correctness, and problem complexity across all puzzle environments. Our analysis from reasoning\n",
      "traces also further validates three regimes of complexity discussed above. For simpler problems,\n",
      "reasoning models often find the correct solution early in their thinking but then continue exploring\n",
      "incorrect solutions. Note the distribution of incorrect solutions (red) is shifted more upward towards\n",
      "end of thinking compared to correct solutions (green). This phenomenon, referred to as “overthinking”\n",
      "in the literature, leads to the waste of compute. As problems become moderately more complex,\n",
      "this trend reverses: models first explore incorrect solutions and mostly later in thought arrive at\n",
      "the correct ones. This time the distribution of incorrect solutions (red) is shifted more downward\n",
      "compared to correct ones (green). Finally, for the problems with higher complexity, collapse emerges,\n",
      "9\n",
      "\n",
      "(a)\n",
      "0\n",
      "4000\n",
      "8000\n",
      "12000\n",
      "Position in Thinking (Token)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Solution Accuracy (%)\n",
      "Tower of Hanoi\n",
      "N=1\n",
      "N=2\n",
      "N=3\n",
      "N=4\n",
      "N=5\n",
      "N=6\n",
      "N=7\n",
      "N=8\n",
      "N=10\n",
      "(b)\n",
      "Figure 7: Left & Middle: Position and correctness of intermediate solutions within reasoning traces\n",
      "across four puzzles at varying complexity levels. ✓indicates correct solutions, ✗indicates incorrect\n",
      "solutions, with distribution density shown by shading; Right: Solution accuracy versus position\n",
      "in thinking for Tower of Hanoi at different complexity levels. Simple problems (N=1-3) show early\n",
      "accuracy declining over time (overthinking), moderate problems (N=4-7) show slight improvement\n",
      "in accuracy with continued reasoning, and complex problems (N≥8) exhibit consistently near-zero\n",
      "accuracy, indicating complete reasoning failure.\n",
      "meaning that the model fails to generate any correct solutions within the thought.\n",
      "Fig. 7b presents a complementary analysis of solution accuracy within sequential segments (bins)\n",
      "of the thoughts in the Tower of Hanoi environment. It can be observed that for simpler problems\n",
      "(smaller N), solution accuracy tends to decrease or oscillate as thinking progresses, providing further\n",
      "evidence of the overthinking phenomenon. However, this trend changes for more complex problems,\n",
      "where solution accuracy increases with thinking progression—up to a certain threshold. Beyond this\n",
      "complexity threshold, in the “collapse mode”, accuracy is zero.\n",
      "4.4\n",
      "Open Questions: Puzzling Behavior of Reasoning Models\n",
      "In this section, we present surprising results concerning the limitations of reasoning models in\n",
      "executing exact problem-solving steps, as well as demonstrating different behaviors of the models\n",
      "based on the number of moves.\n",
      "As shown in Figures 8a and 8b, in the Tower of Hanoi environment, even when we provide the\n",
      "algorithm in the prompt—so that the model only needs to execute the prescribed steps—performance\n",
      "does not improve, and the observed collapse still occurs at roughly the same point. This is noteworthy\n",
      "because finding and devising a solution should require substantially more computation (e.g., for search\n",
      "and verification) than merely executing a given algorithm. This further highlights the limitations of\n",
      "reasoning models in verification and in following logical steps to solve a problem, suggesting that\n",
      "further research is needed to understand the symbolic manipulation capabilities of such models [44, 6].\n",
      "Moreover, in Figures 8c and 8d, we observe very different behavior from the Claude 3.7 Sonnet think-\n",
      "ing model. In the Tower of Hanoi environment, the model’s first error in the proposed solution often\n",
      "occurs much later, e.g., around move 100 for (N=10), compared to the River Crossing environment,\n",
      "where the model can only produce a valid solution until move 4. Note that this model also achieves\n",
      "near-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while\n",
      "it fails to solve the River Crossing puzzle when (N=3), which has a solution of 11 moves. This likely\n",
      "suggests that examples of River Crossing with N>2 are scarce on the web, meaning LRMs may not\n",
      "have frequently encountered or memorized such instances during training.\n",
      "10\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Tower of Hanoi\n",
      "DeepSeek-R1\n",
      "Algorithm Given\n",
      "Default\n",
      "(a)\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Tower of Hanoi\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "Algorithm Given\n",
      "Default\n",
      "(b)\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "First Wrong Move (Median)\n",
      "Tower of Hanoi\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "(c)\n",
      "2 3 4 5 6\n",
      "8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (Number of People)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "First Wrong Move (Median)\n",
      "River Crossing\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "(d)\n",
      "Figure 8: (a) & (b) Despite providing the solution algorithm in the prompt, execution failure\n",
      "occurs at similar points, highlighting reasoning model limitations in logical step execution. (c) &\n",
      "(d) Notably, the Claude 3.7 Sonnet model demonstrates much longer error-free sequences in the\n",
      "Tower of Hanoi compared to early errors in the River Crossing scenario.\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we systematically examine frontier Large Reasoning Models (LRMs) through the lens\n",
      "of problem complexity using controllable puzzle environments. Our findings reveal fundamental\n",
      "limitations in current models: despite sophisticated self-reflection mechanisms, these models fail to\n",
      "develop generalizable reasoning capabilities beyond certain complexity thresholds. We identified\n",
      "three distinct reasoning regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at\n",
      "moderate complexity, and both collapse at high complexity. Particularly concerning is the counterin-\n",
      "tuitive reduction in reasoning effort as problems approach critical complexity, suggesting an inherent\n",
      "compute scaling limit in LRMs. Our detailed analysis of reasoning traces further exposed complexity-\n",
      "dependent reasoning patterns, from inefficient “overthinking” on simpler problems to complete failure\n",
      "on complex ones. These insights challenge prevailing assumptions about LRM capabilities and\n",
      "suggest that current approaches may be encountering fundamental barriers to generalizable reasoning.\n",
      "Finally, we presented some surprising results on LRMs that lead to several open questions for future\n",
      "work. Most notably, we observed their limitations in performing exact computation; for example,\n",
      "when we provided the solution algorithm for the Tower of Hanoi to the models, their performance\n",
      "on this puzzle did not improve. Moreover, investigating the first failure move of the models revealed\n",
      "surprising behaviors. For instance, they could perform up to 100 correct moves in the Tower of\n",
      "Hanoi but fail to provide more than 5 correct moves in the River Crossing puzzle. We believe our\n",
      "results can pave the way for future investigations into the reasoning capabilities of these systems.\n",
      "Limitations\n",
      "We acknowledge that our work has limitations. While our puzzle environments enable controlled\n",
      "experimentation with fine-grained control over problem complexity, they represent a narrow slice of\n",
      "reasoning tasks and may not capture the diversity of real-world or knowledge-intensive reasoning\n",
      "problems. It is notable that most of our experiments rely on black-box API access to the closed frontier\n",
      "LRMs, limiting our ability to analyze internal states or architectural components. Furthermore, the\n",
      "use of deterministic puzzle simulators assumes that reasoning can be perfectly validated step by\n",
      "step. However, in less structured domains, such precise validation may not be feasible, limiting the\n",
      "transferability of this analysis to other more generalizable reasoning.\n",
      "11\n",
      "\n",
      "Acknowledgments\n",
      "The authors would like to thank Scott Hoang, Yichen Jiang, Minsik Cho, Mohammad Sekhavat, David\n",
      "Harrison, Mohammadreza Armandpour and Devi Krishna for the valuable feedback and support.\n",
      "References\n",
      "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\n",
      "Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv\n",
      "preprint arXiv:2412.16720, 2024.\n",
      "[2] OpenAI. Introducing openai o1. Jan 2024.\n",
      "[3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n",
      "Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\n",
      "via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n",
      "[4] Anthropic. Claude 3.7 sonnet. Feb 2025.\n",
      "[5] Google. Gemini flash thinking. Google AI Blog, Jan 2025.\n",
      "[6] Seyed Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio,\n",
      "and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical\n",
      "reasoning in large language models. In The Thirteenth International Conference on Learning\n",
      "Representations, 2025.\n",
      "[7] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2:\n",
      "A new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025.\n",
      "[8] Gary Marcus. Five ways in which the last 3 months — and especially the deepseek era — have\n",
      "vindicated \"deep learning is hitting a wall\". Marcus on AI (Substack), February 2025. Blog\n",
      "post.\n",
      "[9] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany\n",
      "Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, and et. al. Phi-3\n",
      "technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219,\n",
      "2024.\n",
      "[10] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-\n",
      "lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\n",
      "Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\n",
      "Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825,\n",
      "2023.\n",
      "[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\n",
      "Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\n",
      "Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\n",
      "Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière,\n",
      "Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\n",
      "Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\n",
      "Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\n",
      "12\n",
      "\n",
      "Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\n",
      "Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\n",
      "Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,\n",
      "Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,\n",
      "Hu Xu, Hugo Touvron, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024.\n",
      "[12] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean\n",
      "Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal,\n",
      "Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. Faith and fate: Limits of\n",
      "transformers on compositionality. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\n",
      "Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\n",
      "36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New\n",
      "Orleans, LA, USA, December 10 - 16, 2023, 2023.\n",
      "[13] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths.\n",
      "Embers of autoregression: Understanding large language models through the problem they are\n",
      "trained to solve, 2023.\n",
      "[14] Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland:\n",
      "Simple tasks showing complete reasoning breakdown in state-of-the-art large language models.\n",
      "arXiv preprint arXiv:2406.02061, 2024.\n",
      "[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n",
      "Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\n",
      "models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,\n",
      "editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural\n",
      "Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\n",
      "December 9, 2022, 2022.\n",
      "[16] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. Lam-\n",
      "bada:\n",
      "Backward chaining for automated reasoning in natural language.\n",
      "arXiv preprint\n",
      "arXiv:2212.13894, 2022.\n",
      "[17] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie\n",
      "Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066,\n",
      "2022.\n",
      "[18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n",
      "language models are zero-shot reasoners. Advances in neural information processing systems,\n",
      "35:22199–22213, 2022.\n",
      "[19] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and\n",
      "Jun Zhao. Large language models are better reasoners with self-verification. In Houda Bouamor,\n",
      "Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics:\n",
      "EMNLP 2023, pages 2550–2575, Singapore, December 2023. Association for Computational\n",
      "Linguistics.\n",
      "[20] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.\n",
      "Making language models better reasoners with step-aware verifier. In Proceedings of the 61st\n",
      "Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n",
      "pages 5315–5333, 2023.\n",
      "13\n",
      "\n",
      "[21] Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective\n",
      "inference-time search by scaling verification. arXiv preprint arXiv:2502.01839, 2025.\n",
      "[22] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning\n",
      "with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\n",
      "Advances in Neural Information Processing Systems, 2022.\n",
      "[23] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and\n",
      "Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens.\n",
      "In The Twelfth International Conference on Learning Representations, 2024.\n",
      "[24] David Herel and Tomas Mikolov. Thinking tokens for language modeling. ArXiv, abs/2405.08644,\n",
      "2024.\n",
      "[25] Zhihong Shao, Peiyi Wang, Runxin Xu Qihao Zhu, Junxiao Song, Mingchuan Zhang, Y.K. Li,\n",
      "Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\n",
      "language models, 2024.\n",
      "[26] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,\n",
      "Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning\n",
      "through refined credit assignment, 2024.\n",
      "[27] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\n",
      "Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya\n",
      "Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris\n",
      "Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Ha-\n",
      "jishirzi. Tülu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124,\n",
      "2024.\n",
      "[28] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John\n",
      "Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. Reasoning models\n",
      "don’t always say what they think. arXiv preprint arXiv:2505.05410, 2025.\n",
      "[29] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh\n",
      "Hakhamaneshi, Shishir G Patil, Matei Zaharia, et al. Llms can easily learn to reason from\n",
      "demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025.\n",
      "[30] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\n",
      "Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the\n",
      "overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024.\n",
      "[31] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\n",
      "Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient reasoning\n",
      "for large language models. arXiv preprint arXiv:2503.16419, 2025.\n",
      "[32] Sara Vera Marjanović,\n",
      "Arkil Patel,\n",
      "Vaibhav Adlakha,\n",
      "Milad Aghajohari,\n",
      "Parishad\n",
      "BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han\n",
      "Lù, et al. Deepseek-r1 thoughtology: Let’s< think> about llm reasoning. arXiv preprint\n",
      "arXiv:2504.07128, 2025.\n",
      "[33] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching,\n",
      "Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement\n",
      "fine-tuning. arXiv preprint arXiv:2503.07572, 2025.\n",
      "14\n",
      "\n",
      "[34] Marthe Ballon, Andres Algaba, and Vincent Ginis. The relationship between reasoning and\n",
      "performance in large language models–o3 (mini) thinks harder, not longer. arXiv preprint\n",
      "arXiv:2502.15631, 2025.\n",
      "[35] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does\n",
      "reinforcement learning really incentivize reasoning capacity in llms beyond the base model?\n",
      "arXiv preprint arXiv:2504.13837, 2025.\n",
      "[36] Benjamin Estermann, Luca A. Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer.\n",
      "Puzzles: A benchmark for neural algorithmic reasoning, 2024.\n",
      "[37] Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati.\n",
      "Large language models still can’t plan (A benchmark for llms on planning and reasoning about\n",
      "change). CoRR, abs/2206.10498, 2022.\n",
      "[38] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still can’t plan; can\n",
      "lrms? a preliminary evaluation of openai’s o1 on planbench. 2024.\n",
      "[39] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning\n",
      "models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025.\n",
      "[40] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\n",
      "Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\n",
      "arXiv:2305.20050, 2023.\n",
      "[41] Mathematical\n",
      "Association\n",
      "of\n",
      "America.\n",
      "American\n",
      "invitational\n",
      "math-\n",
      "ematics\n",
      "examination\n",
      "(aime).\n",
      "https://maa.org/math-competitions/\n",
      "american-invitational-mathematics-examination-aime, 2025. Accessed: 2025-05-15.\n",
      "[42] Art\n",
      "of\n",
      "Problem\n",
      "Solving.\n",
      "Amc\n",
      "historical\n",
      "results\n",
      "-\n",
      "aime\n",
      "i\n",
      "(february\n",
      "1,\n",
      "2024).\n",
      "https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_\n",
      "I_.28February_1.2C_2024.29, 2024. Accessed: 2025-05-15.\n",
      "[43] Art\n",
      "of\n",
      "Problem\n",
      "Solving.\n",
      "Amc\n",
      "historical\n",
      "results\n",
      "–\n",
      "aime\n",
      "i\n",
      "(february\n",
      "6,\n",
      "2025).\n",
      "https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_\n",
      "I_.28February_6.2C_2025.29, 2025. Accessed: 2025-05-15.\n",
      "[44] Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT\n",
      "press, 2003.\n",
      "[45] Saul Amarel. On representations of problems of reasoning about actions. In Readings in artificial\n",
      "intelligence, pages 2–22. Elsevier, 1981.\n",
      "[46] Günter Rote. Crossing the bridge at night. Bulletin of the EATCS, 78:241, 2002.\n",
      "15\n",
      "\n",
      "A\n",
      "Appendix\n",
      "In this appendix, we provide details supplementing the main text, including experimental setup\n",
      "specifications, additional results, and extended analysis.\n",
      "A.1 Details on Puzzle Environment Specifications and Design - Comprehensive descriptions of all\n",
      "four puzzle environments, including their problem descriptions, prompt designs, and simulators.\n",
      "A.1.1 Tower of Hanoi\n",
      "A.1.2 Checker Jumping\n",
      "A.1.3 River Crossing\n",
      "A.1.4 Blocks World\n",
      "A.2 Implementation Details - Full experimental setup specifications, model configurations, extrac-\n",
      "tion pipeline details, and prescribed algorithm execution experiments.\n",
      "A.3 Details on Computational Complexity\n",
      "A.3.1 Compositional Depth Characterization\n",
      "A.3.2 Performance vs Compositional Depth\n",
      "A.4 Additional Results and Analysis - Extended analysis including reasoning effort patterns, and\n",
      "detailed failure analysis across all models and puzzle environments.\n",
      "A.1\n",
      "Details on Puzzle Environment Specifications and Design\n",
      "A.1.1\n",
      "Tower of Hanoi\n",
      "Problem Description.\n",
      "The Tower of Hanoi is a classic recursive puzzle that serves as a great\n",
      "problem for evaluating sequential reasoning and planning capabilities in reasoning models. The\n",
      "puzzle consists of three pegs (labeled 0, 1, and 2 from left to right) and N disks of varying sizes,\n",
      "where each disk is uniquely numbered from 1 (smallest) to N (largest). In the initial configuration,\n",
      "all N disks are stacked on the leftmost peg (peg 0) in descending order of size, with the largest disk\n",
      "at the bottom and the smallest at the top. The remaining two pegs (1 and 2) are initially empty.\n",
      "The goal is to transfer all disks from peg 0 to peg 2, maintaining the same size ordering (largest\n",
      "at bottom, smallest at top). This puzzle is governed by three fundamental constraints: (1) Single\n",
      "Disk Movement: Only one disk may be moved at a time; (2) Top Disk Access: Only the topmost\n",
      "disk from any peg can be selected for movement; and (3) Size Ordering Constraint: A larger disk\n",
      "may never be placed on top of a smaller disk. This puzzle is a good evaluation testbed for reasoning\n",
      "and planning capabilities of models as it requires models to demonstrate key cognitive demands\n",
      "such as breaking down the problem into subproblems (recursive thinking), tracking multiple states\n",
      "and disk positions simultaneously (working memory management), adhering to movement rules and\n",
      "constraints while planning ahead (constraint satisfaction), and determining the correct order of\n",
      "operations to achieve the final goal (sequential planning).\n",
      "The minimum number of moves required to solve the Tower of Hanoi recursive puzzle with N disks\n",
      "is 2N −1, making it an exponentially scaling problem. This property allows for fine-grained difficulty\n",
      "control by adjusting the problem size with number of initial disks. However, in our evaluation\n",
      "framework, we focus on solution correctness rather than optimality, assessing each of the move’s\n",
      "validity and the model’s ability to reach the target state as the success criteria.\n",
      "16\n",
      "\n",
      "Prompt Design.\n",
      "The system prompt begins with a clear problem statement describing the puzzle\n",
      "setup. It explicitly states the movement rules and the objective of transferring all disks to the third\n",
      "peg. To facilitate understanding, the prompt includes example demonstrations as well as the critical\n",
      "formatting and reasoning expectations.\n",
      "System Prompt - Tower of Hanoi\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "There are three pegs and n disks of different sizes stacked on the first peg. The disks are\n",
      "numbered from 1 (smallest) to n (largest). Disk moves in this puzzle should follow:\n",
      "1. Only one disk can be moved at a time.\n",
      "2. Each move consists of taking the upper disk from one stack and placing it on top of\n",
      "another stack.\n",
      "3. A larger disk may not be placed on top of a smaller disk.\n",
      "The goal is to move the entire stack to the third peg.\n",
      "Example: With 3 disks numbered 1 (smallest), 2, and 3 (largest), the initial state is [[3, 2, 1],\n",
      "[], []], and a solution might be:\n",
      "moves = [[1, 0, 2], [2, 0, 1], [1, 2, 1], [3, 0, 2],\n",
      "[1, 1, 0], [2, 1, 2], [1, 0, 2]]\n",
      "This means: Move disk 1 from peg 0 to peg 2, then move disk 2 from peg 0 to peg 1, and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• The positions are 0-indexed (the leftmost peg is 0).\n",
      "• Ensure your final answer includes the complete list of moves in the format:\n",
      "moves = [[disk id, from peg, to peg], ...]\n",
      "The user prompt after the system prompt presents the specific puzzle instance with current configu-\n",
      "ration showing the distribution of disks across pegs and the goal configuration specifying the target\n",
      "state.\n",
      "User Prompt Template for $N$ Disks - Tower of Hanoi\n",
      "I have a puzzle with $N$ disks of different sizes with\n",
      "Initial configuration:\n",
      "• Peg 0: $N$ (bottom), . . . 2, 1 (top)\n",
      "• Peg 1: (empty)\n",
      "• Peg 2: (empty)\n",
      "17\n",
      "\n",
      "Goal configuration:\n",
      "• Peg 0: (empty)\n",
      "• Peg 1: (empty)\n",
      "• Peg 2: $N$ (bottom), . . . 2, 1 (top)\n",
      "Rules:\n",
      "• Only one disk can be moved at a time.\n",
      "• Only the top disk from any stack can be moved.\n",
      "• A larger disk may not be placed on top of a smaller disk.\n",
      "Find the sequence of moves to transform the initial configuration into the goal configuration.\n",
      "Simulator.\n",
      "Our evaluation framework employs separate puzzle simulators for each puzzle to\n",
      "ensure rigorous and consistent assessment of solutions obtained from LRMs. The Tower of Hanoi\n",
      "simulator is designed as a stateful environment that tracks disk configurations across three pegs\n",
      "and validates each proposed move against the puzzle’s fundamental constraints. The simulator\n",
      "architecture follows a modular design pattern with clear separation between state management,\n",
      "move validation, and solution verification. In this simulator, we have a puzzle class which tracks the\n",
      "current disk configuration and enforces the puzzle’s fundamental constraints. We also have a method\n",
      "to execute each move in the puzzle setup and perform four-layer validation: checking peg boundary\n",
      "conditions (0-2), verifying source pegs contain disks, confirming the specified disk is topmost, and\n",
      "enforcing the size ordering constraint that prevents larger disks from being placed on smaller ones.\n",
      "Upon successful validation, the method executes the disk transfer and updates the game state. Then,\n",
      "the complete solution validation is processed by sequentially processing move lists, and verifying\n",
      "goal state achievement.\n",
      "A.1.2\n",
      "Checker Jumping\n",
      "Problem Description.\n",
      "Checker Jumping is a one-dimensional constraint-satisfaction puzzle\n",
      "designed to test sequential reasoning, planning, and rule understanding capabilities. The puzzle\n",
      "consists of a linear arrangement of red checkers (’R’), blue checkers (’B’), and a single empty space\n",
      "(’_’). In the standard configuration, N red checkers are positioned on the left side, followed by an\n",
      "empty space in the middle, and N blue checkers on the right side, forming a linear board of length\n",
      "2N + 1. The objective is to swap the positions of all red and blue checkers, effectively mirroring the\n",
      "initial configuration, where red checkers end up on the right and blue checkers on the left. Movement\n",
      "in this puzzle is governed by two fundamental rules: (1) Slide Movement: A checker can slide\n",
      "forward into an adjacent empty space; and (2) Jump Movement: A checker can jump forward over\n",
      "exactly one checker of the opposite color to land in an empty space. Therefore, checkers cannot\n",
      "move backward toward their starting side—red checkers can only move rightward, and blue checkers\n",
      "can only move leftward from the initial configuration. This puzzle presents cognitive challenges that\n",
      "make it a great testbed for reasoning models. For example, models must demonstrate some aspect of\n",
      "spatial reasoning (tracking checker positions and possible moves), constraint satisfaction (adhering\n",
      "to movement rules during puzzle), lookahead planning (anticipating how current moves affect future\n",
      "18\n",
      "\n",
      "possibilities towards goal), and state-space exploration (searching through possible move sequences\n",
      "to find a valid solution path).\n",
      "The difficulty of the Checker Jumping puzzle scales with the number of checkers: with N checkers of\n",
      "each color, the minimum solution requires (N + 1)2 −1 moves, creating a quadratic relationship\n",
      "between problem size and solution complexity. In our evaluation framework, we mainly focus on\n",
      "solution correctness rather than optimality, evaluating each move against the puzzle constraints and\n",
      "confirming that the final state matches the goal configuration. This approach allows us to precisely\n",
      "identify reasoning failures and constraint violations that might occur during the solution process.\n",
      "Prompt Design.\n",
      "The system prompt begins with a clear problem statement describing the puzzle\n",
      "setup and movement rules. It explicitly states the objective and provides a concrete example with a\n",
      "small board configuration to illustrate how moves should be represented.\n",
      "System Prompt - Checker Jumping\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "On a one-dimensional board, there are red checkers (’R’), blue checkers (’B’), and one empty\n",
      "space (’_’). A checker can move by either:\n",
      "1. Sliding forward into an adjacent empty space, or\n",
      "2. Jumping over exactly one checker of the opposite color to land in an empty space.\n",
      "The goal is to swap the positions of all red and blue checkers, effectively mirroring the initial\n",
      "state.\n",
      "Example: If the initial state is [’R’, ’_’, ’B’], the goal is to reach [’B’, ’_’, ’R’]. Your solution\n",
      "should be a list of moves where each move is represented as [checker_color, position_from,\n",
      "position_to]. For example:\n",
      "moves = [[’R’, 0, 1], [’B’, 2, 0], [’R’, 1, 2]]\n",
      "This means: Move the red checker from position 0 to 1, then move the blue checker from\n",
      "position 2 to 0, and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• The positions are 0-indexed (the leftmost position is 0).\n",
      "• Ensure your final answer includes the complete list of moves for final solution in the\n",
      "format: moves = [[checker_color, position_from, position_to], ...]\n",
      "The user prompt presents the specific puzzle instance with the initial board configuration, and the\n",
      "goal state.\n",
      "19\n",
      "\n",
      "User Prompt Template for $N$ Checkers - Checker Jumping\n",
      "I have a puzzle with 2$N$+1 positions, where $N$ red checkers (’R’) on left, $N$ blue checkers\n",
      "(’B’) on right, and one empty space (’_’) in between are arranged in a line.\n",
      "Initial board: R R ... R _ B B ... B\n",
      "Goal board: B B ... B _ R R ... R\n",
      "Rules:\n",
      "• A checker can slide into an adjacent empty space.\n",
      "• A checker can jump over exactly one checker of the opposite color to land in an empty\n",
      "space.\n",
      "• Checkers cannot move backwards (towards their starting side).\n",
      "Find the minimum sequence of moves to transform the initial board into the goal board.\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating Checker Jumping\n",
      "puzzle solutions. The simulator implements a comprehensive validation system that enforces all\n",
      "puzzle constraints while tracking the state evolution throughout the solution path. The Checker\n",
      "Jumping simulator is designed as a stateful environment that tracks the position of all checkers and\n",
      "the empty space, validating each move of a given solution against the puzzle’s movement rules. The\n",
      "simulator begins by validating that both the initial and goal states are well-formed, containing the\n",
      "same number of red and blue checkers and exactly one empty space. Then, each move is executed\n",
      "with a method that performs multi-layer validation: verifying position boundaries, confirming correct\n",
      "checker color at source, ensuring target positions are empty, and validating move types as either\n",
      "slides (distance=1) or jumps (distance=2). The simulator enforces directional constraints preventing\n",
      "backward movement (red checkers move right, blue checkers move left) and validates jump moves\n",
      "by confirming the presence of an opposite-colored checker in the middle position. Upon successful\n",
      "validation, the method executes the checker transfer by updating positions and clearing the source.\n",
      "Then, the complete move sequences are processed with final goal state verification.\n",
      "A.1.3\n",
      "River Crossing\n",
      "Problem Description.\n",
      "River Crossing is a constraint satisfaction planning puzzle that tests multi-\n",
      "agent coordination and constraint management. This puzzle is a generalization of classic problems\n",
      "such as the Missionaries and Cannibals problem and the Bridge and Torch problem, which have been\n",
      "widely studied in planning literature [45, 46]. The river crossing puzzle involves N actors (denoted by\n",
      "a1, a2, ..., aN) and their corresponding N agents (denoted by A1, A2, ..., AN) who must cross a river us-\n",
      "ing a boat. In the initial state, all 2N individuals are on the left bank of the river. The goal is to trans-\n",
      "port everyone safely to the right bank. The puzzle operates under several key movement constraints:\n",
      "(1) Boat Capacity Constraint: The boat can carry at most k individuals at a time, where k is typically\n",
      "set to 2 for smaller puzzles (N ≤3) and 3 for larger puzzles (N ≤5); (2) Non-Empty Boat Constraint:\n",
      "The boat cannot travel empty and must have at least one person aboard; (3) Safety Constraint: An\n",
      "actor cannot be in the presence of another agent unless their own agent is also present, as agents must\n",
      "protect their clients from competing agents. This safety constraint applies both on the banks and in\n",
      "the boat. This puzzle requires complex planning and state tracking as participants must carefully coor-\n",
      "dinate their crossings while maintaining safety constraints at all times. The solver must reason through\n",
      "20\n",
      "\n",
      "different combinations of individuals who can safely travel together, determine who should return with\n",
      "the boat after a crossing, and strategically plan a sequence that eventually brings everyone to the right\n",
      "bank without violating any constraints. The complexity of this task can be controlled by adjusting the\n",
      "number of actor-agent pairs and the boat capacity, creating a scalable challenge for reasoning models.\n",
      "Prompt Design.\n",
      "The system prompt introduces the notation for representing actors and agents,\n",
      "establishes the solution format as a list of boat moves, and provides a simple example to demonstrate\n",
      "the format.\n",
      "System Prompt - River Crossing\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "You can represent actors with a_1, a_2, ... and agents with A_1, A_2, ... . Your solution\n",
      "must be a list of boat moves where each move indicates the people on the boat. For example, if\n",
      "there were two actors and two agents, you should return:\n",
      "moves =[[\" A_2\", \"a_2\"], [\"A_2\"], [\"A_1\", \"A_2\"], [\"A_1\"], [\"A_1\", \"a_1\n",
      "\"]]\n",
      "which indicates that in the first move, A_2 and a_2 row from left to right, and in the second\n",
      "move, A_2 rows from right to left and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of boat moves.\n",
      "• The list shouldn’t have comments.\n",
      "• Ensure your final answer also includes the complete list of moves for final solution.\n",
      "The user prompt presents the specific puzzle instance with N actor-agent pairs, and the boat capacity\n",
      "k, and the safety constraint that must be maintained throughout the solution.\n",
      "User Prompt Template for $N$ Pairs - River Crossing\n",
      "$N$ actors and their $N$ agents want to cross a river in a boat that is capable of holding\n",
      "only $k$ people at a time, with the constraint that no actor can be in the presence\n",
      "of another agent, including while riding the boat, unless their own agent is also\n",
      "present, because each agent is worried their rivals will poach their client. Initially, all actors\n",
      "and agents are on the left side of the river with the boat. How should they cross the river?\n",
      "(Note: the boat cannot travel empty)\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating River Crossing\n",
      "puzzle extracted solutions. The simulator tracks the state of all individuals (actors and agents) and\n",
      "the boat position while enforcing all puzzle constraints. Each move is executed with multi-step\n",
      "validation: checking boat capacity limits, verifying all passengers are on the boat’s current side,\n",
      "and enforcing the critical safety constraint that actors cannot be in the presence of other agents\n",
      "without their own agent present, both on the boat and on each bank after the move. The simulator\n",
      "21\n",
      "\n",
      "manages dynamic boat positioning, automatically switching sides after each crossing, and validates\n",
      "the complete state after each move to ensure no safety violations occur on either bank. Then, the\n",
      "complete crossing sequences are verified that all 2N individuals successfully reach the right bank.\n",
      "A.1.4\n",
      "Blocks World\n",
      "Problem Description.\n",
      "Blocks World is a classical planning puzzle that has been recently studied\n",
      "for analyzing the planning capabilities of LLMs [37, 38]. The puzzle involves multiple stacks of blocks\n",
      "(A, B, C, etc.) that must be rearranged from an initial configuration to a specified goal configuration.\n",
      "Each block is uniquely identified by its letter, and the objective is to find the minimum sequence of\n",
      "moves needed to transform the initial state into the goal state. The puzzle operates only under two\n",
      "fundamental constraints: (1) Top Block Movement: Only the topmost block from any stack can be\n",
      "moved; and (2) Valid Placement: A block can only be placed either on an empty position or on top\n",
      "of another block. These constraints create planning problem where the order of operations becomes\n",
      "critical, as some configurations may require temporary placement of blocks to access those beneath\n",
      "them later. Blocks World serves as a great testbed for evaluating planning capabilities in reasoning\n",
      "models because it requires forward thinking, and state tracking. Recent studies have examined this\n",
      "puzzle in various configurations, including simplified settings with as few as 3 to 5 blocks, to evaluate\n",
      "LLM performance on sequential planning tasks [37, 38]. Models must demonstrate the ability to\n",
      "decompose complex state transformations into valid sequential moves, reason about dependencies\n",
      "between blocks (e.g., unblocking lower blocks before accessing them), and efficiently plan paths to\n",
      "the goal state without illegal moves.\n",
      "The difficulty of this puzzle can be scaled by adjusting several parameters: the number of blocks, the\n",
      "number of stacks, and the complexity of the initial and goal configurations. We primarily control\n",
      "complexity through the block count N, while following clear structural patterns in the initial and\n",
      "goal configurations. In our experimental design, the initial configuration consistently divides the\n",
      "N blocks between two stacks in alphabetical order, with the third stack empty as workspace. The\n",
      "goal configuration consolidates all blocks onto the first stack in a systematic interleaved pattern\n",
      "that alternates between blocks from the two initial stacks, with specific positioning that requires\n",
      "complete disassembly and reassembly of the existing stacks. For example, for N = 4, the initial\n",
      "state has blocks divided between two stacks [[\"A\", \"B\"], [\"C\", \"D\"], []] and the goal state\n",
      "[[\"D\", \"B\", \"C\", \"A\"], [], []] requires interleaving blocks from both stacks; and for N = 6,\n",
      "the initial state [[\"A\", \"B\", \"C\"], [\"D\", \"E\", \"F\"], []] must be transformed to [[\"F\", \"C\",\n",
      "\"E\", \"B\", \"D\", \"A\"], [], []], forming a complex alternating pattern. As N increases, the state\n",
      "space grows factorially, and the minimum solution length increases approximately linearly with\n",
      "N. For small values of N (2-7), the puzzles test basic planning; for medium values (8-20), they\n",
      "require more complex reasoning with longer planning horizons; and for large values (N > 20), they\n",
      "challenge the limits of sequential reasoning capabilities by requiring extensive temporary movements\n",
      "and pattern recognition across lengthy solution paths.\n",
      "Prompt Design.\n",
      "The system prompt introduces the fundamental rules of the Blocks World puzzle,\n",
      "establishes the move representation format, and provides a simple example to demonstrate the\n",
      "solution structure.\n",
      "22\n",
      "\n",
      "System Prompt - Blocks World\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "In this puzzle, there are stacks of blocks, and the goal is to rearrange them into a target\n",
      "configuration using a sequence of moves where:\n",
      "• Only the topmost block from any stack can be moved.\n",
      "• A block can be placed either on an empty position or on top of another block.\n",
      "Example:\n",
      "With initial state [[\"A\", \"B\"], [\"C\"], []] and goal state [[\"A\"], [\"B\"],\n",
      "[\"C\"]], a solution might be:\n",
      "moves = [[\"C\", 1, 2], [\"B\", 0, 1]]\n",
      "This means: Move block C from stack 1 to stack 2, then move block B from stack 0 to stack 1.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• Ensure your final answer also includes the complete list of moves for final solution in the\n",
      "format: moves = [[block, from stack, to stack], ...]\n",
      "The user prompt presents the specific puzzle instance with the initial and goal configurations provided,\n",
      "and explicitly reminds the model about the movement constraint.\n",
      "User Prompt Template for $N$ Blocks - BlocksWorld\n",
      "I have a puzzle with $N$ blocks.\n",
      "Initial state:\n",
      "Stack 0: $blocks_0$ (top)\n",
      "Stack 1: $blocks_1$ (top)\n",
      "...\n",
      "Stack $m$: $blocks_m$ (top)\n",
      "Goal state:\n",
      "Stack 0: $goal_blocks_0$ (top)\n",
      "Stack 1: $goal_blocks_1$ (top)\n",
      "...\n",
      "Stack $m$: $goal_blocks_m$ (top)\n",
      "Find the minimum sequence of moves to transform the initial state into the goal state. Remember\n",
      "that only the topmost block of each stack can be moved.\n",
      "23\n",
      "\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating Blocks World\n",
      "puzzle extracted solutions. The simulator manages the state of all blocks across stacks while enforcing\n",
      "the puzzle’s movement constraints. Each move is executed in the puzzle setup with three-layer\n",
      "validation: verifying stack indices are within bounds, confirming the source stack contains blocks,\n",
      "and ensuring the specified block is at the top of its stack (enforcing the top-block-only movement\n",
      "rule). Upon successful validation, the block transfer is executed and the block is popped from the\n",
      "source stack and appended to the destination stack. Finally, the complete solution sequences of block\n",
      "movements are processed and verified that the resulting configuration matches the target goal state.\n",
      "A.2\n",
      "Implementation Details\n",
      "Configurations\n",
      "Our experiments primarily utilized reasoning models and their non-thinking\n",
      "counterparts to enable thorough analysis of the thinking process. We specifically selected Claude\n",
      "3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3 due to their ability to provide access to\n",
      "thinking traces, a critical requirement for our analysis. For experiments focused solely on final\n",
      "accuracy metrics, we also included results from OpenAI’s o3-mini models, as they lack access to\n",
      "thoughts. For Claude 3.7 Sonnet (thinking and non-thinking) models we used maximum generation\n",
      "budget of 64,000 tokens, accessed through the API interface. Temperature is set to 1.0 for all API\n",
      "rus (Claude-3.7-Sonnet and o3-mini runs). The experiments with DeepSeek-R1, DeepSeek-V3, and\n",
      "DeepSeek-R1-Distill-Qern-32B are conducted on local servers with maximum generation length set to\n",
      "64,000 and temperature set to 1.0. In all experiments, we generated 25 samples per puzzle instance\n",
      "at each complexity level (N value) and reported performance averages across all samples.\n",
      "Solution Extraction\n",
      "A custom extraction pipeline was developed to process model responses\n",
      "and intermediate reasoning traces (thoughts). The pipeline consists of several key components. We\n",
      "implemented a flexible regex-based extractors to identify potential solution attempts in both the\n",
      "final response and thinking trace. The extraction process identify solution patterns using regular\n",
      "expressions (both explicit “moves =” patterns and alternative bracket-based solutions). We process\n",
      "and clean each extracted candidate solution by (i) Removing comments from the list (text following\n",
      "\"#\" in any line), and (ii) Normalizing move formats to what suggested in context to ensure consistent\n",
      "structure. Then, we validate solution format and structure to filter out invalid matches. During\n",
      "the extraction, we also capture metadata of token position for each extracted solution. Notably, for\n",
      "accurate position tracking within thinking traces, we employed the same tokenizer (cl100k_base)\n",
      "as the corresponding model to count tokens across all experiments. Token positions were also\n",
      "normalized with respect to thought length to enable cross-sample comparison. Finally, we make sure\n",
      "that the recorded solutions within the thought trace are unique and duplicate solutions (identical\n",
      "moves list) were filtered. In case of duplicate solutions, only the first solution is recorded for analysis.\n",
      "Solution Evaluation\n",
      "After extraction, each solution candidate is passed to the corresponding\n",
      "simulator of puzzle for fine-grained verification. The simulator takes a solution as list of moves and\n",
      "evaluate that with respect to the puzzle (check App. A.1 for details of each puzzle simulator). Each\n",
      "move in the compositional solution is executed sequentially according to previous moves and the\n",
      "puzzle rules. Then, the final state obtained from all moves in the sequence is compared to the goal\n",
      "state of puzzle to determine full solution correctness. For incorrect solutions, details of first failure\n",
      "move and the type of failure is also collected during the move verification with puzzle simulator.\n",
      "Execution of Prescribed Steps\n",
      "In addition to open-ended problem solving across different\n",
      "puzzles, we also conducted focused experiments to test how providing the explicit solving algorithm\n",
      "24\n",
      "\n",
      "guidance with prescribed steps would affect behavior of these reasoning models (Sec. 4.4).\n",
      "We expected that finding and devising solution from scratch should require substantially more\n",
      "computation for model (e.g., for search and verification) than just following a given algorithm’s\n",
      "steps. However, results in Figures 8a and 8b show that reasoning models’ behavior does not change\n",
      "that much and the collapse still occurs at roughly same points as before with this setting. This\n",
      "finding strengthens evidence that the limitation is not just in problem-solving and solution strategy\n",
      "discovery but also in consistent logical verification and step execution limitation throughout the\n",
      "generated reasoning chains.\n",
      "For example, models are provided with a complete recursive algorithm of solving Tower of Hanoi\n",
      "puzzle as follows. This algorithm scratchpad was appended to the standard problem prompt to test\n",
      "its impact on reasoning behavior.\n",
      "Example of Prescribed Algorithm for Tower of Hanoi\n",
      "Here is a pseudocode of recursive algorithm to solve the puzzle:\n",
      "ALGORITHM Solve(n, source, target, auxiliary, moves)\n",
      "// n = number of disks to move\n",
      "// source = starting peg (0, 1, or 2)\n",
      "// target = destination peg (0, 1, or 2)\n",
      "// auxiliary = the unused peg (0, 1, or 2)\n",
      "// moves = list to store the sequence of moves\n",
      "IF n equals 1 THEN\n",
      "// Get the top disk from source peg\n",
      "disk = the top disk on the source peg\n",
      "// Add the move to our list: [disk_id, source, target]\n",
      "ADD [disk, source, target] to moves\n",
      "RETURN\n",
      "END IF\n",
      "// Move n-1 disks from source to auxiliary peg\n",
      "Solve(n-1, source, auxiliary, target, moves)\n",
      "// Move the nth disk from source to target\n",
      "disk = the top disk on the source peg\n",
      "ADD [disk, source, target] to moves\n",
      "// Move n-1 disks from auxiliary to target\n",
      "Solve(n-1, auxiliary, target, source, moves)\n",
      "END ALGORITHM\n",
      "To solve the entire puzzle of moving n disks from peg 0 to peg 2:\n",
      "1. Initialize an empty list ’moves’\n",
      "2. Execute Solve(n, 0, 2, 1, moves)\n",
      "3. The ’moves’ list will contain the complete solution\n",
      "25\n",
      "\n",
      "Note: When executing this pseudocode, track which disk is currently on top of each peg. The\n",
      "disk IDs in the moves list should correspond to the actual disk being moved.\n",
      "You can use this algorithm as a scratchpad to help you solve the problem step by step.\n",
      "A.3\n",
      "Details on Computational Complexity\n",
      "A.3.1\n",
      "Compositional Depth Characterization\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Problem Size (N)\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Compositional Depth (# of Moves)\n",
      "Blocks World\n",
      "Checker Jumping\n",
      "River Crossing\n",
      "Tower of Hanoi\n",
      "Figure 9: Compositional depth (number of\n",
      "moves required) across different problem sizes\n",
      "for our four puzzle environments.\n",
      "Compositional depth is the number of sequential op-\n",
      "erations (i.e., moves) required to reach a full solution.\n",
      "Figure 9 demonstrates how this depth scales with\n",
      "problem size (N) across our four puzzle environments.\n",
      "Each puzzle has a distinct growth pattern, reflecting\n",
      "its underlying computational complexity. For exam-\n",
      "ple, Tower of Hanoi shows exponential growth (2N−1),\n",
      "and Checker Jumping displays quadratic scaling (\n",
      "(N + 1)2 −1). The River Crossing and Blocks World\n",
      "puzzles show more moderate, near-linear growth with\n",
      "N. These varying compositional depth profiles enable\n",
      "us to evaluate how language reasoning models handle\n",
      "different types of sequential reasoning challenges and\n",
      "if their accuracy is always correlated with the com-\n",
      "positional depth required to solve the puzzle. More\n",
      "details regarding this analysis is provided in Figure 10\n",
      "in App. A.4.\n",
      "A.3.2\n",
      "Performance vs Compositional Depth\n",
      "While intuition suggests a negative correlation between problem complexity and model accuracy, our\n",
      "analysis reveals a more nuanced relationship between compositional depth and LRM performance.\n",
      "Figure 10 demonstrates this across three state-of-the-art reasoning models (Claude-3.7-Sonnet w.\n",
      "thinking, DeepSeek-R1, and o3-mini) on our puzzle suite. Within individual puzzle types, we observe\n",
      "the expected negative correlation: as compositional depth increases, model accuracy consistently\n",
      "decreases. However, across different puzzle types, this relation breaks. Models may struggle with\n",
      "puzzles of lower compositional depth while succeeding on different puzzles with higher compositional\n",
      "depth. . For instance, models achieve >50% accuracy on Tower of Hanoi instances requiring\n",
      "approximately 102 moves, yet consistently fail on River Crossing puzzles with substantially lower\n",
      "compositional depth (∼101 moves).\n",
      "A.4\n",
      "Extended Results and Analysis\n",
      "Failure Analysis.\n",
      "Understanding where models fail within the compositional reasoning steps\n",
      "provides insights beyond binary success metrics. Our accuracy evaluation requires perfect execution\n",
      "of entire move sequences—a single incorrect move results in failure. To examine failure patterns\n",
      "more granularly, we analyze the compositional depth at which models first make incorrect moves\n",
      "across varying problem complexity levels.\n",
      "26\n",
      "\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "DeepSeek-R1\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "o3-mini (high)\n",
      "Tower Hanoi\n",
      "Checker Jumping\n",
      "River Crossing\n",
      "Blocks World\n",
      "Figure 10: Accuracy versus compositional depth (number of moves required) for three LRMs\n",
      "(DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across four puzzle environments.\n",
      "Figure 11 shows the failure move ID versus problem complexity (N) within the solution sequence.\n",
      "The top row compares Claude-3.7-Sonnet with and without thinking capabilities, while the bottom\n",
      "row compares DeepSeek-R1 (thinking) with DeepSeek-V3 (non-thinking).\n",
      "These comparisons\n",
      "demonstrates how thinking mechanisms of LRMs influence failure patterns in compositional reasoning\n",
      "tasks of puzzles. Several counterintuitive patterns emerge from our analysis. First, models exhibit\n",
      "non-monotonic failure behavior with respect to problem complexity—instances where models fail\n",
      "earlier in the solution sequence for higher N values despite requiring longer overall solutions. For\n",
      "example, in Tower of Hanoi, models sometimes fail at below 50 moves for N = 15 but succeed through\n",
      "more than 100 moves for N = 8, contradicting the expectation that effective algorithmic planning\n",
      "and execution for the same puzzle should maintain consistent failure patterns relative to solution\n",
      "progress. This suggests fundamental inconsistencies in how models (both LRMs and their non-\n",
      "thinking standard LLM counterparts) apply learned solution strategies across different problem scales.\n",
      "Also, we observe that in the high-complexity regimes where both model variants experience complete\n",
      "accuracy collapse, e.g., Tower of Hanoi with N ≥15 and Blocks World with N ≥40, non-thinking\n",
      "models occasionally sustain performance deeper into the solution sequence and are able to fail at later\n",
      "moves than thinking-enabled variants. This is interesting as it shows that compositional reasoning\n",
      "failures in LLMs are not simply due to insufficient context length or inference compute, but rather\n",
      "reflect fundamental limitations in how models maintain algorithmic consistency across problem scales.\n",
      "We also analyze the distributional characteristics of failure moves to understand the consistency and\n",
      "reliability of model reasoning. Figure 12 presents the density distributions of failure move positions\n",
      "aggregated across all problem complexities for each puzzle environment, comparing thinking and\n",
      "non-thinking models within the same family. Based on the figure, thinking models (Claude-3.7-Sonnet\n",
      "with thinking and DeepSeek-R1) consistently show higher mean failure positions across all puzzles,\n",
      "as indicated by the dashed vertical lines showing mean of first failure in sequence of moves. However,\n",
      "the distribution shape of thinking models mostly have higher variance in their failure patterns. This\n",
      "suggests that while these models can reach deeper into solution sequences on average, their reasoning\n",
      "processes are more instable and prone to inconsistent performance.\n",
      "Reasoning Effort Dynamics.\n",
      "Figure 13 demonstrates the reasoning effort (measured by inference\n",
      "thinking tokens) versus problem complexity across our puzzle environments. Green dots indicate\n",
      "27\n",
      "\n",
      "Figure 11: The first failure move versus problem complexity (N) comparison for thinking and\n",
      "non-thinking models across puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom:\n",
      "DeepSeek-R1 vs DeepSeek-V3.\n",
      "correct solutions, red crosses show incorrect ones, and blue lines track average thinking token usage at\n",
      "each complexity level (N) across different puzzles and LRMs. We observe a consistent pattern across\n",
      "all three reasoning models (DeepSeek-R1, Claude-3.7-Sonnet-thinking, o3-mini) where thinking token\n",
      "usage, i.e. reasoning effort, initially scales with problem complexity but counterintuitively declines\n",
      "after reaching a model-specific threshold. This suggests an interesting and fundamental scaling limit\n",
      "in LRM thinking process for reasoning where beyond certain complexity thresholds, models not\n",
      "only fail to solve problems but counterintuitively reduce their inference compute despite facing more\n",
      "difficult problems and being well below the context and generation limits.\n",
      "28\n",
      "\n",
      "Figure 12: Density distribution of first failure moves for thinking and non-thinking models across\n",
      "puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom: DeepSeek-R1 vs DeepSeek-V3.\n",
      "29\n",
      "\n",
      "Figure 13: Detailed results on reasoning effort (measured in inference thinking tokens) versus problem\n",
      "complexity (N) for three LRMs (DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across\n",
      "four puzzle environments.\n",
      "30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "query = \"What is the architecture described?\"\n",
    "query_embedding = model.encode(\"query: \" + query)\n",
    "query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "context = chunks[I[0][0]]\n",
    "\n",
    "print(\"Most Relevant Context:\\n\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54feb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Context:\n",
      " The Illusion of Thinking:\n",
      "Understanding the Strengths and Limitations of Reasoning Models\n",
      "via the Lens of Problem Complexity\n",
      "Parshin Shojaee∗†\n",
      "Iman Mirzadeh∗\n",
      "Keivan Alizadeh\n",
      "Maxwell Horton\n",
      "Samy Bengio\n",
      "Mehrdad Farajtabar\n",
      "Apple\n",
      "Abstract\n",
      "Recent generations of frontier language models have introduced Large Reasoning Models\n",
      "(LRMs) that generate detailed thinking processes before providing answers. While these models\n",
      "demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal-\n",
      "ing properties, and limitations remain insufficiently understood. Current evaluations primarily fo-\n",
      "cus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How-\n",
      "ever, this evaluation paradigm often suffers from data contamination and does not provide insights\n",
      "into the reasoning traces’ structure and quality. In this work, we systematically investigate these\n",
      "gaps with the help of controllable puzzle environments that allow precise manipulation of composi-\n",
      "tional complexity while maintaining consistent logical structures. This setup enables the analysis\n",
      "of not only final answers but also the internal reasoning traces, offering insights into how LRMs\n",
      "“think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs\n",
      "face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-\n",
      "intuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then\n",
      "declines despite having an adequate token budget. By comparing LRMs with their standard LLM\n",
      "counterparts under equivalent inference compute, we identify three performance regimes: (1) low-\n",
      "complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity\n",
      "tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks\n",
      "where both models experience complete collapse. We found that LRMs have limitations in exact\n",
      "computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We\n",
      "also investigate the reasoning traces in more depth, studying the patterns of explored solutions\n",
      "and analyzing the models’ computational behavior, shedding light on their strengths, limitations,\n",
      "and ultimately raising crucial questions about their true reasoning capabilities.\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have recently evolved to include specialized variants explicitly\n",
      "designed for reasoning tasks—Large Reasoning Models (LRMs) such as OpenAI’s o1/o3 [1, 2],\n",
      "DeepSeek-R1 [3], Claude 3.7 Sonnet Thinking [4], and Gemini Thinking [5]. These models are new\n",
      "artifacts, characterized by their “thinking” mechanisms such as long Chain-of-Thought (CoT) with\n",
      "self-reflection, and have demonstrated promising results across various reasoning benchmarks. Their\n",
      "∗Equal contribution.\n",
      "†Work done during an internship at Apple.\n",
      "{p_shojaee, imirzadeh, kalizadehvahid, mchorton, bengio, farajtabar}@apple.com\n",
      "1\n",
      "\n",
      "Initial State\n",
      "Middle State\n",
      "1\n",
      "Peg 0\n",
      "[1, 0, 2]\n",
      "[2, 0, 1]\n",
      "[1, 2, 1]\n",
      "[3, 0, 2]\n",
      "[1, 1, 0]\n",
      "[2, 1, 2]\n",
      "[1, 0, 2]\n",
      "Peg 1\n",
      "Peg 2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "Target State\n",
      "<think>\n",
      "Move disk 1 from peg 0 to peg 2 ...  \n",
      "moves = [\n",
      "]\n",
      "Let me double-check this...\n",
      "</think>\n",
      "  [1, 0, 2],\n",
      "  [2, 0, 1],\n",
      "  [1, 2, 1],\n",
      "  [3, 0, 2],\n",
      "  [1, 1, 0],\n",
      "  [2, 1, 2],\n",
      "  [1, 0, 2],\n",
      "<answer> the final answer is moves=... \n",
      "</answer>\n",
      "LLM Response\n",
      "extract moves  from thoughts \n",
      "(for analysis)\n",
      "extract final answer \n",
      "(for measuring accuracy)\n",
      "1 2 3 4 5 6 7 8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (number of disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Claude 3.7\n",
      "(+thinking)\n",
      "Claude 3.7\n",
      "1 2 3 4 5 6 7 8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (number of disks)\n",
      "0\n",
      "5,000\n",
      "10,000\n",
      "15,000\n",
      "20,000\n",
      "Response Length (Tokens)\n",
      "Claude 3.7\n",
      "(+thinking)\n",
      "Claude 3.7\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "Complexity (number of disks)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Position within Thoughts\n",
      "Correct Solutions\n",
      "Incorrect Solutions\n",
      "Figure 1: Top: Our setup enables verification of both final answers and intermediate reasoning traces,\n",
      "allowing detailed analysis of model thinking behavior. Bottom left & middle: At low complexity,\n",
      "non-thinking models are more accurate and token-efficient. As complexity increases, reasoning models\n",
      "outperform but require more tokens—until both collapse beyond a critical threshold, with shorter\n",
      "traces.\n",
      "Bottom right: For correctly solved cases, Claude 3.7 Thinking tends to find answers early\n",
      "at low complexity and later at higher complexity. In failed cases, it often fixates on an early wrong\n",
      "answer, wasting the remaining token budget. Both cases reveal inefficiencies in the reasoning process.\n",
      "emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning\n",
      "and problem-solving tasks, with some researchers proposing them as significant steps toward more\n",
      "general artificial intelligence capabilities.\n",
      "Despite these claims and performance advancements, the fundamental benefits and limitations of\n",
      "LRMs remain insufficiently understood. Critical questions still persist: Are these models capable\n",
      "of generalizable reasoning, or are they leveraging different forms of pattern matching [6]? How\n",
      "does their performance scale with increasing problem complexity? How do they compare to their\n",
      "non-thinking standard LLM counterparts when provided with the same inference token compute?\n",
      "Most importantly, what are the inherent limitations of current reasoning approaches, and what\n",
      "improvements might be necessary to advance toward more robust reasoning capabilities?\n",
      "We believe the lack of systematic analyses investigating these questions is due to limitations in\n",
      "current evaluation paradigms. Existing evaluations predominantly focus on established mathematical\n",
      "and coding benchmarks, which, while valuable, often suffer from data contamination issues and do\n",
      "not allow for controlled experimental conditions across different settings and complexities. Moreover,\n",
      "these evaluations do not provide insights into the structure and quality of reasoning traces. To\n",
      "understand the reasoning behavior of these models more rigorously, we need environments that\n",
      "enable controlled experimentation.\n",
      "In this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem\n",
      "2\n",
      "\n",
      "complexity. Rather than standard benchmarks (e.g., math problems), we adopt controllable puzzle en-\n",
      "vironments that let us vary complexity systematically—by adjusting puzzle elements while preserving\n",
      "the core logic—and inspect both solutions and internal reasoning (Fig. 1, top). These puzzles: (1) of-\n",
      "fer fine-grained control over complexity; (2) avoid contamination common in established benchmarks;\n",
      "(3) require only the explicitly provided rules, emphasizing algorithmic reasoning; and (4) support\n",
      "rigorous, simulator-based evaluation, enabling precise solution checks and detailed failure analyses.\n",
      "Our empirical investigation reveals several key findings about current Language Reasoning Models\n",
      "(LRMs): First, despite their sophisticated self-reflection mechanisms learned through reinforcement\n",
      "learning, these models fail to develop generalizable problem-solving capabilities for planning tasks,\n",
      "with performance collapsing to zero beyond a certain complexity threshold. Second, our comparison\n",
      "between LRMs and standard LLMs under equivalent inference compute reveals three distinct reason-\n",
      "ing regimes (Fig. 1, bottom). For simpler, low-compositional problems, standard LLMs demonstrate\n",
      "greater efficiency and accuracy. As problem complexity moderately increases, thinking models gain\n",
      "an advantage. However, when problems reach high complexity with longer compositional depth,\n",
      "both model types experience complete performance collapse (Fig. 1, bottom left). Notably, near\n",
      "this collapse point, LRMs begin reducing their reasoning effort (measured by inference-time tokens)\n",
      "as problem complexity increases, despite operating well below generation length limits (Fig. 1,\n",
      "bottom middle). This suggests a fundamental inference time scaling limitation in LRMs’ reasoning\n",
      "capabilities relative to problem complexity. Finally, our analysis of intermediate reasoning traces or\n",
      "thoughts reveals complexity-dependent patterns: In simpler problems, reasoning models often identify\n",
      "correct solutions early but inefficiently continue exploring incorrect alternatives—an “overthinking”\n",
      "phenomenon. At moderate complexity, correct solutions emerge only after extensive exploration\n",
      "of incorrect paths. Beyond a certain complexity threshold, models completely fail to find correct\n",
      "solutions (Fig. 1, bottom right). This indicates LRMs possess limited self-correction capabilities\n",
      "that, while valuable, reveal fundamental inefficiencies and clear scaling limitations.\n",
      "These findings highlight both the strengths and limitations of existing LRMs, raising questions\n",
      "about the nature of reasoning in these systems with important implications for their design and\n",
      "deployment. Our key contributions are:\n",
      "• We question the current evaluation paradigm of LRMs on established math benchmarks and\n",
      "design a controlled experimental testbed by leveraging algorithmic puzzle environments that enable\n",
      "controllable experimentation with respect to problem complexity.\n",
      "• We show that state-of-the-art LRMs (e.g., o3-mini, DeepSeek-R1, Claude-3.7-Sonnet-Thinking)\n",
      "still fail to develop generalizable problem-solving capabilities, with accuracy ultimately collapsing\n",
      "to zero beyond certain complexities across different environments.\n",
      "• We find that there exists a scaling limit in the LRMs’ reasoning effort with respect to problem\n",
      "complexity, evidenced by the counterintuitive decreasing trend in the thinking tokens after a\n",
      "complexity point.\n",
      "• We question the current evaluation paradigm based on final accuracy and extend our evaluation\n",
      "to intermediate solutions of thinking traces with the help of deterministic puzzle simulators. Our\n",
      "analysis reveals that as problem complexity increases, correct solutions systematically emerge at\n",
      "later positions in thinking compared to incorrect ones, providing quantitative insights into the\n",
      "self-correction mechanisms within LRMs.\n",
      "• We uncover surprising limitations in LRMs’ ability to perform exact computation, including their\n",
      "failure to benefit from explicit algorithms and their inconsistent reasoning across puzzle types.\n",
      "3\n",
      "\n",
      "2\n",
      "Related Works\n",
      "Reasoning in Language Models.\n",
      "Large Language Models (LLMs) undergo multiple costly\n",
      "training phases using vast amounts of training data. While these LLMs demonstrate promising\n",
      "language understanding with strong compression capabilities, their intelligence and reasoning abilities\n",
      "remain a critical topic of scientific debate [7, 8]. Earlier iterations of LLMs [9, 10, 11] exhibited\n",
      "poor performance on reasoning benchmarks [12, 13, 14, 6]. To address these shortcomings, several\n",
      "approaches have been explored with the common theme among them being “scaling” both the training\n",
      "data and test-time computation. For instance, generating a Chain of Thought (CoT) [15, 16, 17, 18]\n",
      "and incorporating self-verification [19, 20, 21] prior to the final answer have been shown to improve\n",
      "model performance. However, obtaining high-quality and scalable CoT data is quite expensive\n",
      "due to its scarcity. Another line of research focuses on compensating for the lack of supervised\n",
      "data by teaching models to think more effectively through supervised learning or reinforcement\n",
      "learning [22, 23, 24, 25, 26, 27]. A notable open-source example of these improvements is Deepseek-\n",
      "R1 [3], which demonstrated that applying RL with verifiable rewards can significantly enhance model\n",
      "performance, matching that of closed models like OpenAI’s o1 [2], leading to a new generation of\n",
      "language models referred to as Large Reasoning Models (LRMs) such as Gemini flash thinking [5],\n",
      "Claude 3.7 Sonnet thinking [4], etc.\n",
      "Understanding Large Reasoning Models.\n",
      "Recent studies have explored various aspects of\n",
      "reasoning behavior: Large Reasoning Models have shown emergent behaviors such as discrepancy\n",
      "between thought traces and final answers [28, 29] as well as efficiency concerns through what\n",
      "researchers term the “overthinking phenomenon” [30, 31, 32, 33], where models produce verbose,\n",
      "redundant outputs, even after finding the solution, creating significant inference computational\n",
      "overhead. In this work, we systematically analyze how much model thinks w.r.t task complexity.\n",
      "Recently, Ballon et al. [34] demonstrated that in newer LRMs accuracy generally declines when\n",
      "thinking increases in math problems, in contrast we observe when in controlled puzzle environment\n",
      "difficulty passes a certain level the model starts to think less and opposite corelation of thinking and\n",
      "task complexity only happens up to some threshold. Yue et al. [35] questioned whether reinforcement\n",
      "learning truly elicits novel reasoning patterns and shows pass@k of reasoning vs non-reasoning models\n",
      "converge to the same point. We also observe that in MATH-500 pass@k is close for reasoning versus\n",
      "non-reasoning models but we observed different patterns under medium and high complexity of\n",
      "puzzles, which is not easily observable on established math benchmarks used in common evaluations.\n",
      "Controllable Evaluation Environments.\n",
      "Unlike earlier studies that focused on mathematical\n",
      "problems to evaluate the reasoning capabilities of language models, this work introduces controllable\n",
      "puzzle environments. These environments allow for precise manipulation of problem complexity while\n",
      "maintaining consistent logical processes, enabling a more rigorous analysis of reasoning patterns and\n",
      "limitations. Controllable environments are not uncommon in the literature [12, 36, 37]. However,\n",
      "our primary aim is not to propose a new benchmark; instead, we use these benchmarks as tools\n",
      "for designing experiments to understand the reasoning capabilities of language models. A closely\n",
      "related study by Valmeekam et al. [38] demonstrated that o1-models show significant performance\n",
      "improvements compared to previous models. Our work offers additional insights, such as examining\n",
      "pairs of thinking/non-thinking models (e.g., DeepSeek-R1/V3, Claude 3.7 Sonnet thinking/non-\n",
      "thinking). Furthermore, we study the reasoning traces of the LRMs in more depth, revealing different\n",
      "behaviors across various complexity levels.\n",
      "Overall, the promising results from recent LRMs raise a critical question: how much have the\n",
      "previously reported limitations of LLMs been improved? In this work, we move beyond merely\n",
      "measuring the performance of these LRMs. We analyze how well these LRMs tackle problems of\n",
      "varying complexities and examine the properties of their reasoning processes.\n",
      "4\n",
      "\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "Inference Compute Budget (Tokens)\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "pass@k\n",
      "MATH-500\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME24\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME25\n",
      "claude-3-7-sonnet-thinking\n",
      "claude-3-7-sonnet-no-thinking\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "Inference Compute Budget (Tokens)\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "pass@k\n",
      "MATH-500\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME24\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "0\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "Inference Compute Budget (Tokens)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "pass@k\n",
      "AIME25\n",
      "DeepSeek-R1\n",
      "DeepSeek-V3\n",
      "Figure 2: Comparative analysis of thinking versus non-thinking models across math benchmarks\n",
      "reveals inconsistent performance patterns. While results on the MATH-500 dataset show comparable\n",
      "performance between both model types, the thinking models demonstrate superior performance\n",
      "on AIME24 and AIME25 benchmarks. Additionally, the observed performance degradation from\n",
      "AIME24 to AIME25 highlights the vulnerability of these benchmarks to data contamination issues.\n",
      "3\n",
      "Math and Puzzle Environments\n",
      "Currently, it is not clear whether the performance enhancements observed in recent RL-based\n",
      "thinking models are attributable to increased exposure to established mathematical benchmark\n",
      "data, to the significantly greater inference compute allocated to thinking tokens, or to reasoning\n",
      "capabilities developed by RL-based training? Recent studies [35, 39] have explored this question\n",
      "with established math benchmarks by comparing the upper-bound capabilities (pass@k) of RL-based\n",
      "thinking models with their non-thinking standard LLM counterparts. They have shown that under\n",
      "equivalent inference token budgets, non-thinking LLMs can eventually reach performance comparable\n",
      "to thinking models on benchmarks like MATH500 [40] and AIME24 [41]. We also conducted our\n",
      "comparative analysis of frontier LRMs like Claude-3.7-Sonnet (with vs. without thinking) and\n",
      "DeepSeek (R1 vs. V3). Our results (shown in Fig. 2) confirm that, on the MATH500 dataset, the\n",
      "pass@k performance of thinking models is comparable to their non-thinking counterparts when\n",
      "provided with the same inference token budget. However, we observed that this performance gap\n",
      "widens on the AIME24 benchmark and widens further on AIME25. This widening gap presents\n",
      "an interpretive challenge. It could be attributed to either: (1) increasing complexity requiring\n",
      "more sophisticated reasoning processes, thus revealing genuine advantages of the thinking models\n",
      "for more complex problems, or (2) reduced data contamination in newer benchmarks (particularly\n",
      "AIME25). Interestingly, human performance on AIME25 was actually higher than on AIME24\n",
      "[42, 43], suggesting that AIME25 might be less complex. Yet models perform worse on AIME25\n",
      "than AIME24—potentially suggesting data contamination during the training of frontier LRMs.\n",
      "Given these non-justified observations and the fact that mathematical benchmarks do not allow for\n",
      "controlled manipulation of problem complexity, we turned to puzzle environments that enable more\n",
      "precise and systematic experimentation.\n",
      "5\n",
      "\n",
      "Initial State\n",
      "Middle State\n",
      "moves\n",
      "moves\n",
      "Tower of Hanoi\n",
      "Checkers Jumping\n",
      "River Crossing\n",
      "Blocks World\n",
      "Target State\n",
      "Figure 3: Illustration of the four puzzle environments. Columns show the progression from initial\n",
      "state (top) through intermediate state (middle) to target state (bottom) for puzzles: Tower\n",
      "of Hanoi (disk transfer across pegs), Checkers Jumping (position swapping of colored tokens), River\n",
      "Crossing (transporting entities across a river), and Blocks World (stack reconfiguration).\n",
      "3.1\n",
      "Puzzle Environments\n",
      "We evaluate LRM reasoning on four controllable puzzles spanning compositional depth, planning\n",
      "complexity, and distributional settings. The puzzles are defined below and illustrated in Fig. 3.\n",
      "Tower of Hanoi is a puzzle featuring three pegs and n disks of different sizes stacked on the first\n",
      "peg in size order (largest at bottom). The goal is to transfer all disks from the first peg to the third\n",
      "peg. Valid moves include moving only one disk at a time, taking only the top disk from a peg, and\n",
      "never placing a larger disk on top of a smaller one. The difficulty in this task can be controlled by\n",
      "the number of initial disks as the minimum number of required moves with n initial disks will be\n",
      "2n −1. However, in this work we do not grade for optimality of final solution and only measuring\n",
      "the correctness of each move and reaching the target state.\n",
      "Checker Jumping is a one-dimensional puzzle arranging red checkers, blue checkers, and a single\n",
      "empty space in a line. The objective is to swap the positions of all red and blue checkers, effectively\n",
      "mirroring the initial configuration. Valid moves include sliding a checker into an adjacent empty\n",
      "space or jumping over exactly one checker of the opposite color to land in an empty space. No checker\n",
      "can move backward in the puzzle process. The complexity of this task can be controlled by the\n",
      "number of checkers: with 2n checkers, the minimum number of moves required will be (n + 1)2 −1.\n",
      "River Crossing is a constraint satisfaction planning puzzle involving n actors and their corresponding\n",
      "n agents who must cross a river using a boat. The goal is to transport all 2n individuals from the\n",
      "left bank to the right bank. The boat can carry at most k individuals and cannot travel empty.\n",
      "Invalid situations arise when an actor is in the presence of another agent without their own agent\n",
      "present, as each agent must protect their client from competing agents. The complexity of this task\n",
      "can also be controlled by the number of actor/agent pairs present. For n = 2, n = 3 pairs, we use\n",
      "boat capacity of k = 2 and for larger number of pairs we use k = 3.\n",
      "Blocks World is a block-stacking puzzle requiring rearrangement of blocks from an initial configu-\n",
      "ration into a specified goal configuration. The objective is to find the minimum number of moves\n",
      "needed for this transformation. Valid moves are restricted to the topmost block of any stack, which\n",
      "can be placed either on an empty stack or on top of another block. The complexity in this task can\n",
      "be controlled by the number of blocks present.\n",
      "6\n",
      "\n",
      "Figure 4: Accuracy of thinking models (Claude 3.7 Sonnet with thinking, DeepSeek-R1) versus their\n",
      "non-thinking counterparts (Claude 3.7 Sonnet, DeepSeek-V3) across all puzzle environments and\n",
      "varying levels of problem complexity.\n",
      "4\n",
      "Experiments & Results\n",
      "4.1\n",
      "Experimental Setup\n",
      "Most of our experiments are conducted on reasoning models and their non-thinking counterparts,\n",
      "such as Claude 3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3. We chose these models\n",
      "because they allow access to the thinking tokens, unlike models such as OpenAI’s o-series. For\n",
      "experiments focused solely on final accuracy, we also report results on the o-series models. For Claude\n",
      "3.7 Sonnet models, we allow the maximum token budget (64k). Similarly, for DeepSeek-R1/V3\n",
      "models on local servers, we allow the maximum length to be up to 64k tokens. For each puzzle\n",
      "instance, we generate 25 samples and report the average performance of each model across them.\n",
      "Comprehensive details of our experimental setup and results are provided in the Appendix.\n",
      "4.2\n",
      "How Does Complexity Affect Reasoning?\n",
      "4.2.1\n",
      "Three Regimes of Complexity\n",
      "Motivated by the observations in Fig. 2, to systematically investigate the impact of problem complexity\n",
      "on reasoning behavior, we conducted experiments comparing thinking and non-thinking model\n",
      "pairs across our controlled puzzle environments. Our analysis focused on matched pairs of LLMs\n",
      "with identical model backbones, specifically Claude-3.7-Sonnet (w. vs. w/o thinking) and DeepSeek\n",
      "(R1 vs. V3). In each puzzle, we vary the complexity by manipulating problem size N (representing\n",
      "disk count, checker count, block count, or crossing elements).\n",
      "Fig. 4 presents the accuracy of both model types as a function of problem complexity across all\n",
      "puzzle environments. Complementing this, Fig. 5 shows the upper bound performance capabilities\n",
      "(pass@k) of these model pairs under equivalent inference token compute (averaged across all puzzles),\n",
      "extending earlier analyses from mathematical benchmarks (Fig. 2) to the controlled puzzle environ-\n",
      "ments. Results from both these figures demonstrate that, unlike observations from math, there exists\n",
      "three regimes in the behavior of these models with respect to complexity. In the first regime where\n",
      "problem complexity is low, we observe that non-thinking models are capable to obtain performance\n",
      "comparable to, or even better than thinking models with more token-efficient inference. In the\n",
      "7\n",
      "\n",
      "Figure 5: Pass@k performance of thinking vs. non-thinking models across equivalent compute\n",
      "budgets in puzzle environments of low , medium , and high complexity. Non-thinking models excel\n",
      "in simple problems, thinking models show advantages at medium complexity, while both approaches\n",
      "fail at high complexity regardless of compute allocation.\n",
      "second regime with medium complexity, the advantage of reasoning models capable of generating\n",
      "long chain-of-thought begin to manifest, and the performance gap between model pairs increases. The\n",
      "most interesting regime is the third regime where problem complexity is higher and the performance\n",
      "of both models have collapsed to zero. Results show that while thinking models delay this collapse,\n",
      "they also ultimately encounter the same fundamental limitations as their non-thinking counterparts.\n",
      "4.2.2\n",
      "Collapse of Reasoning Models\n",
      "We next examine how different specialized reasoning models equipped with thinking tokens respond\n",
      "to increasing problem complexity. Our experiments evaluate five state-of-the-art thinking models:\n",
      "o3-mini (medium and high configurations), DeepSeek-R1, DeepSeek-R1-Qwen-32B, and Claude-3.7-\n",
      "Sonnet (thinking). Fig. 6 demonstrates these models’ performance in terms of accuracy (top) and\n",
      "thinking token usage (bottom) across varying complexity levels. Results show that all reasoning\n",
      "models exhibit a similar pattern with respect to complexity: accuracy progressively declines as\n",
      "problem complexity increases until reaching complete collapse (zero accuracy) beyond a model-\n",
      "specific complexity threshold. Analysis of inference thinking token compute also reveals an intriguing\n",
      "pattern in thinking token allocation learned by these models. We observe that reasoning models\n",
      "initially increase their thinking tokens proportionally with problem complexity. However, upon\n",
      "approaching a critical threshold—which closely corresponds to their accuracy collapse point—models\n",
      "counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty. This\n",
      "phenomenon is most pronounced in o3-mini variants and less severe in the Claude-3.7-Sonnet\n",
      "(thinking) model. Notably, despite operating well below their generation length limits with ample\n",
      "inference budget available, these models fail to take advantage of additional inference compute during\n",
      "the thinking phase as problems become more complex. This behavior suggests a fundamental scaling\n",
      "limitation in the thinking capabilities of current reasoning models relative to problem complexity.\n",
      "8\n",
      "\n",
      "Figure 6: Accuracy and thinking tokens vs. problem complexity for reasoning models across puzzle\n",
      "environments. As complexity increases, reasoning models initially spend more tokens while accuracy\n",
      "declines gradually, until a critical point where reasoning collapses—performance drops sharply and\n",
      "reasoning effort decreases.\n",
      "4.3\n",
      "What Happens Inside the Thoughts of Reasoning Models?\n",
      "To gain deeper insights into the thinking processes of reasoning models, we conducted a fine-grained\n",
      "analysis of their reasoning traces. As shown in Fig. 1, our setup with puzzle environments allows us\n",
      "to look beyond final answer and obtain more detailed insight into the reasoning traces (“thoughts”)\n",
      "produced by these models. We extract and analyze the intermediate solutions explored within the\n",
      "thoughts of a model with the help of puzzle simulators. Our investigation examines the patterns and\n",
      "characteristics of these intermediate solutions, their correctness relative to their sequential position\n",
      "in the reasoning process, and how these patterns evolve with increasing problem complexity. For\n",
      "this analysis, we focus on the reasoning traces generated by Claude-3.7-Sonnet-Thinking across\n",
      "our puzzle suite. For each intermediate solution identified within the traces, we recorded: (1) its\n",
      "relative position within the reasoning trace (normalized by total thought length), (2) its correctness\n",
      "as validated by our puzzle simulators, and (3) the complexity of the corresponding problem. This\n",
      "allows to characterize the progression and accuracy of solution development throughout the reasoning\n",
      "process.\n",
      "Fig. 7a demonstrates the relation between the position of intermediate solutions within thoughts, their\n",
      "correctness, and problem complexity across all puzzle environments. Our analysis from reasoning\n",
      "traces also further validates three regimes of complexity discussed above. For simpler problems,\n",
      "reasoning models often find the correct solution early in their thinking but then continue exploring\n",
      "incorrect solutions. Note the distribution of incorrect solutions (red) is shifted more upward towards\n",
      "end of thinking compared to correct solutions (green). This phenomenon, referred to as “overthinking”\n",
      "in the literature, leads to the waste of compute. As problems become moderately more complex,\n",
      "this trend reverses: models first explore incorrect solutions and mostly later in thought arrive at\n",
      "the correct ones. This time the distribution of incorrect solutions (red) is shifted more downward\n",
      "compared to correct ones (green). Finally, for the problems with higher complexity, collapse emerges,\n",
      "9\n",
      "\n",
      "(a)\n",
      "0\n",
      "4000\n",
      "8000\n",
      "12000\n",
      "Position in Thinking (Token)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Solution Accuracy (%)\n",
      "Tower of Hanoi\n",
      "N=1\n",
      "N=2\n",
      "N=3\n",
      "N=4\n",
      "N=5\n",
      "N=6\n",
      "N=7\n",
      "N=8\n",
      "N=10\n",
      "(b)\n",
      "Figure 7: Left & Middle: Position and correctness of intermediate solutions within reasoning traces\n",
      "across four puzzles at varying complexity levels. ✓indicates correct solutions, ✗indicates incorrect\n",
      "solutions, with distribution density shown by shading; Right: Solution accuracy versus position\n",
      "in thinking for Tower of Hanoi at different complexity levels. Simple problems (N=1-3) show early\n",
      "accuracy declining over time (overthinking), moderate problems (N=4-7) show slight improvement\n",
      "in accuracy with continued reasoning, and complex problems (N≥8) exhibit consistently near-zero\n",
      "accuracy, indicating complete reasoning failure.\n",
      "meaning that the model fails to generate any correct solutions within the thought.\n",
      "Fig. 7b presents a complementary analysis of solution accuracy within sequential segments (bins)\n",
      "of the thoughts in the Tower of Hanoi environment. It can be observed that for simpler problems\n",
      "(smaller N), solution accuracy tends to decrease or oscillate as thinking progresses, providing further\n",
      "evidence of the overthinking phenomenon. However, this trend changes for more complex problems,\n",
      "where solution accuracy increases with thinking progression—up to a certain threshold. Beyond this\n",
      "complexity threshold, in the “collapse mode”, accuracy is zero.\n",
      "4.4\n",
      "Open Questions: Puzzling Behavior of Reasoning Models\n",
      "In this section, we present surprising results concerning the limitations of reasoning models in\n",
      "executing exact problem-solving steps, as well as demonstrating different behaviors of the models\n",
      "based on the number of moves.\n",
      "As shown in Figures 8a and 8b, in the Tower of Hanoi environment, even when we provide the\n",
      "algorithm in the prompt—so that the model only needs to execute the prescribed steps—performance\n",
      "does not improve, and the observed collapse still occurs at roughly the same point. This is noteworthy\n",
      "because finding and devising a solution should require substantially more computation (e.g., for search\n",
      "and verification) than merely executing a given algorithm. This further highlights the limitations of\n",
      "reasoning models in verification and in following logical steps to solve a problem, suggesting that\n",
      "further research is needed to understand the symbolic manipulation capabilities of such models [44, 6].\n",
      "Moreover, in Figures 8c and 8d, we observe very different behavior from the Claude 3.7 Sonnet think-\n",
      "ing model. In the Tower of Hanoi environment, the model’s first error in the proposed solution often\n",
      "occurs much later, e.g., around move 100 for (N=10), compared to the River Crossing environment,\n",
      "where the model can only produce a valid solution until move 4. Note that this model also achieves\n",
      "near-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while\n",
      "it fails to solve the River Crossing puzzle when (N=3), which has a solution of 11 moves. This likely\n",
      "suggests that examples of River Crossing with N>2 are scarce on the web, meaning LRMs may not\n",
      "have frequently encountered or memorized such instances during training.\n",
      "10\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Tower of Hanoi\n",
      "DeepSeek-R1\n",
      "Algorithm Given\n",
      "Default\n",
      "(a)\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Tower of Hanoi\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "Algorithm Given\n",
      "Default\n",
      "(b)\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "15\n",
      "20\n",
      "Complexity (Number of Disks)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "First Wrong Move (Median)\n",
      "Tower of Hanoi\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "(c)\n",
      "2 3 4 5 6\n",
      "8\n",
      "10\n",
      "15\n",
      "20\n",
      "Complexity (Number of People)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "First Wrong Move (Median)\n",
      "River Crossing\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "(d)\n",
      "Figure 8: (a) & (b) Despite providing the solution algorithm in the prompt, execution failure\n",
      "occurs at similar points, highlighting reasoning model limitations in logical step execution. (c) &\n",
      "(d) Notably, the Claude 3.7 Sonnet model demonstrates much longer error-free sequences in the\n",
      "Tower of Hanoi compared to early errors in the River Crossing scenario.\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we systematically examine frontier Large Reasoning Models (LRMs) through the lens\n",
      "of problem complexity using controllable puzzle environments. Our findings reveal fundamental\n",
      "limitations in current models: despite sophisticated self-reflection mechanisms, these models fail to\n",
      "develop generalizable reasoning capabilities beyond certain complexity thresholds. We identified\n",
      "three distinct reasoning regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at\n",
      "moderate complexity, and both collapse at high complexity. Particularly concerning is the counterin-\n",
      "tuitive reduction in reasoning effort as problems approach critical complexity, suggesting an inherent\n",
      "compute scaling limit in LRMs. Our detailed analysis of reasoning traces further exposed complexity-\n",
      "dependent reasoning patterns, from inefficient “overthinking” on simpler problems to complete failure\n",
      "on complex ones. These insights challenge prevailing assumptions about LRM capabilities and\n",
      "suggest that current approaches may be encountering fundamental barriers to generalizable reasoning.\n",
      "Finally, we presented some surprising results on LRMs that lead to several open questions for future\n",
      "work. Most notably, we observed their limitations in performing exact computation; for example,\n",
      "when we provided the solution algorithm for the Tower of Hanoi to the models, their performance\n",
      "on this puzzle did not improve. Moreover, investigating the first failure move of the models revealed\n",
      "surprising behaviors. For instance, they could perform up to 100 correct moves in the Tower of\n",
      "Hanoi but fail to provide more than 5 correct moves in the River Crossing puzzle. We believe our\n",
      "results can pave the way for future investigations into the reasoning capabilities of these systems.\n",
      "Limitations\n",
      "We acknowledge that our work has limitations. While our puzzle environments enable controlled\n",
      "experimentation with fine-grained control over problem complexity, they represent a narrow slice of\n",
      "reasoning tasks and may not capture the diversity of real-world or knowledge-intensive reasoning\n",
      "problems. It is notable that most of our experiments rely on black-box API access to the closed frontier\n",
      "LRMs, limiting our ability to analyze internal states or architectural components. Furthermore, the\n",
      "use of deterministic puzzle simulators assumes that reasoning can be perfectly validated step by\n",
      "step. However, in less structured domains, such precise validation may not be feasible, limiting the\n",
      "transferability of this analysis to other more generalizable reasoning.\n",
      "11\n",
      "\n",
      "Acknowledgments\n",
      "The authors would like to thank Scott Hoang, Yichen Jiang, Minsik Cho, Mohammad Sekhavat, David\n",
      "Harrison, Mohammadreza Armandpour and Devi Krishna for the valuable feedback and support.\n",
      "References\n",
      "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\n",
      "Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv\n",
      "preprint arXiv:2412.16720, 2024.\n",
      "[2] OpenAI. Introducing openai o1. Jan 2024.\n",
      "[3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n",
      "Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\n",
      "via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n",
      "[4] Anthropic. Claude 3.7 sonnet. Feb 2025.\n",
      "[5] Google. Gemini flash thinking. Google AI Blog, Jan 2025.\n",
      "[6] Seyed Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio,\n",
      "and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical\n",
      "reasoning in large language models. In The Thirteenth International Conference on Learning\n",
      "Representations, 2025.\n",
      "[7] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2:\n",
      "A new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025.\n",
      "[8] Gary Marcus. Five ways in which the last 3 months — and especially the deepseek era — have\n",
      "vindicated \"deep learning is hitting a wall\". Marcus on AI (Substack), February 2025. Blog\n",
      "post.\n",
      "[9] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany\n",
      "Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, and et. al. Phi-3\n",
      "technical report: A highly capable language model locally on your phone. CoRR, abs/2404.14219,\n",
      "2024.\n",
      "[10] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-\n",
      "lot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\n",
      "Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\n",
      "Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825,\n",
      "2023.\n",
      "[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\n",
      "Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\n",
      "Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\n",
      "Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière,\n",
      "Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\n",
      "Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\n",
      "Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\n",
      "12\n",
      "\n",
      "Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\n",
      "Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\n",
      "Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,\n",
      "Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,\n",
      "Hu Xu, Hugo Touvron, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024.\n",
      "[12] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean\n",
      "Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal,\n",
      "Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. Faith and fate: Limits of\n",
      "transformers on compositionality. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,\n",
      "Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\n",
      "36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New\n",
      "Orleans, LA, USA, December 10 - 16, 2023, 2023.\n",
      "[13] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths.\n",
      "Embers of autoregression: Understanding large language models through the problem they are\n",
      "trained to solve, 2023.\n",
      "[14] Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland:\n",
      "Simple tasks showing complete reasoning breakdown in state-of-the-art large language models.\n",
      "arXiv preprint arXiv:2406.02061, 2024.\n",
      "[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\n",
      "Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\n",
      "models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,\n",
      "editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural\n",
      "Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\n",
      "December 9, 2022, 2022.\n",
      "[16] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. Lam-\n",
      "bada:\n",
      "Backward chaining for automated reasoning in natural language.\n",
      "arXiv preprint\n",
      "arXiv:2212.13894, 2022.\n",
      "[17] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie\n",
      "Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066,\n",
      "2022.\n",
      "[18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n",
      "language models are zero-shot reasoners. Advances in neural information processing systems,\n",
      "35:22199–22213, 2022.\n",
      "[19] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and\n",
      "Jun Zhao. Large language models are better reasoners with self-verification. In Houda Bouamor,\n",
      "Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics:\n",
      "EMNLP 2023, pages 2550–2575, Singapore, December 2023. Association for Computational\n",
      "Linguistics.\n",
      "[20] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.\n",
      "Making language models better reasoners with step-aware verifier. In Proceedings of the 61st\n",
      "Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n",
      "pages 5315–5333, 2023.\n",
      "13\n",
      "\n",
      "[21] Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective\n",
      "inference-time search by scaling verification. arXiv preprint arXiv:2502.01839, 2025.\n",
      "[22] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning\n",
      "with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\n",
      "Advances in Neural Information Processing Systems, 2022.\n",
      "[23] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and\n",
      "Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens.\n",
      "In The Twelfth International Conference on Learning Representations, 2024.\n",
      "[24] David Herel and Tomas Mikolov. Thinking tokens for language modeling. ArXiv, abs/2405.08644,\n",
      "2024.\n",
      "[25] Zhihong Shao, Peiyi Wang, Runxin Xu Qihao Zhu, Junxiao Song, Mingchuan Zhang, Y.K. Li,\n",
      "Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\n",
      "language models, 2024.\n",
      "[26] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy,\n",
      "Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning\n",
      "through refined credit assignment, 2024.\n",
      "[27] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\n",
      "Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya\n",
      "Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris\n",
      "Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Ha-\n",
      "jishirzi. Tülu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124,\n",
      "2024.\n",
      "[28] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John\n",
      "Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. Reasoning models\n",
      "don’t always say what they think. arXiv preprint arXiv:2505.05410, 2025.\n",
      "[29] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh\n",
      "Hakhamaneshi, Shishir G Patil, Matei Zaharia, et al. Llms can easily learn to reason from\n",
      "demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025.\n",
      "[30] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\n",
      "Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the\n",
      "overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024.\n",
      "[31] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\n",
      "Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient reasoning\n",
      "for large language models. arXiv preprint arXiv:2503.16419, 2025.\n",
      "[32] Sara Vera Marjanović,\n",
      "Arkil Patel,\n",
      "Vaibhav Adlakha,\n",
      "Milad Aghajohari,\n",
      "Parishad\n",
      "BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han\n",
      "Lù, et al. Deepseek-r1 thoughtology: Let’s< think> about llm reasoning. arXiv preprint\n",
      "arXiv:2504.07128, 2025.\n",
      "[33] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching,\n",
      "Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement\n",
      "fine-tuning. arXiv preprint arXiv:2503.07572, 2025.\n",
      "14\n",
      "\n",
      "[34] Marthe Ballon, Andres Algaba, and Vincent Ginis. The relationship between reasoning and\n",
      "performance in large language models–o3 (mini) thinks harder, not longer. arXiv preprint\n",
      "arXiv:2502.15631, 2025.\n",
      "[35] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does\n",
      "reinforcement learning really incentivize reasoning capacity in llms beyond the base model?\n",
      "arXiv preprint arXiv:2504.13837, 2025.\n",
      "[36] Benjamin Estermann, Luca A. Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer.\n",
      "Puzzles: A benchmark for neural algorithmic reasoning, 2024.\n",
      "[37] Karthik Valmeekam, Alberto Olmo Hernandez, Sarath Sreedharan, and Subbarao Kambhampati.\n",
      "Large language models still can’t plan (A benchmark for llms on planning and reasoning about\n",
      "change). CoRR, abs/2206.10498, 2022.\n",
      "[38] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still can’t plan; can\n",
      "lrms? a preliminary evaluation of openai’s o1 on planbench. 2024.\n",
      "[39] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning\n",
      "models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025.\n",
      "[40] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\n",
      "Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint\n",
      "arXiv:2305.20050, 2023.\n",
      "[41] Mathematical\n",
      "Association\n",
      "of\n",
      "America.\n",
      "American\n",
      "invitational\n",
      "math-\n",
      "ematics\n",
      "examination\n",
      "(aime).\n",
      "https://maa.org/math-competitions/\n",
      "american-invitational-mathematics-examination-aime, 2025. Accessed: 2025-05-15.\n",
      "[42] Art\n",
      "of\n",
      "Problem\n",
      "Solving.\n",
      "Amc\n",
      "historical\n",
      "results\n",
      "-\n",
      "aime\n",
      "i\n",
      "(february\n",
      "1,\n",
      "2024).\n",
      "https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_\n",
      "I_.28February_1.2C_2024.29, 2024. Accessed: 2025-05-15.\n",
      "[43] Art\n",
      "of\n",
      "Problem\n",
      "Solving.\n",
      "Amc\n",
      "historical\n",
      "results\n",
      "–\n",
      "aime\n",
      "i\n",
      "(february\n",
      "6,\n",
      "2025).\n",
      "https://artofproblemsolving.com/wiki/index.php/AMC_historical_results#AIME_\n",
      "I_.28February_6.2C_2025.29, 2025. Accessed: 2025-05-15.\n",
      "[44] Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT\n",
      "press, 2003.\n",
      "[45] Saul Amarel. On representations of problems of reasoning about actions. In Readings in artificial\n",
      "intelligence, pages 2–22. Elsevier, 1981.\n",
      "[46] Günter Rote. Crossing the bridge at night. Bulletin of the EATCS, 78:241, 2002.\n",
      "15\n",
      "\n",
      "A\n",
      "Appendix\n",
      "In this appendix, we provide details supplementing the main text, including experimental setup\n",
      "specifications, additional results, and extended analysis.\n",
      "A.1 Details on Puzzle Environment Specifications and Design - Comprehensive descriptions of all\n",
      "four puzzle environments, including their problem descriptions, prompt designs, and simulators.\n",
      "A.1.1 Tower of Hanoi\n",
      "A.1.2 Checker Jumping\n",
      "A.1.3 River Crossing\n",
      "A.1.4 Blocks World\n",
      "A.2 Implementation Details - Full experimental setup specifications, model configurations, extrac-\n",
      "tion pipeline details, and prescribed algorithm execution experiments.\n",
      "A.3 Details on Computational Complexity\n",
      "A.3.1 Compositional Depth Characterization\n",
      "A.3.2 Performance vs Compositional Depth\n",
      "A.4 Additional Results and Analysis - Extended analysis including reasoning effort patterns, and\n",
      "detailed failure analysis across all models and puzzle environments.\n",
      "A.1\n",
      "Details on Puzzle Environment Specifications and Design\n",
      "A.1.1\n",
      "Tower of Hanoi\n",
      "Problem Description.\n",
      "The Tower of Hanoi is a classic recursive puzzle that serves as a great\n",
      "problem for evaluating sequential reasoning and planning capabilities in reasoning models. The\n",
      "puzzle consists of three pegs (labeled 0, 1, and 2 from left to right) and N disks of varying sizes,\n",
      "where each disk is uniquely numbered from 1 (smallest) to N (largest). In the initial configuration,\n",
      "all N disks are stacked on the leftmost peg (peg 0) in descending order of size, with the largest disk\n",
      "at the bottom and the smallest at the top. The remaining two pegs (1 and 2) are initially empty.\n",
      "The goal is to transfer all disks from peg 0 to peg 2, maintaining the same size ordering (largest\n",
      "at bottom, smallest at top). This puzzle is governed by three fundamental constraints: (1) Single\n",
      "Disk Movement: Only one disk may be moved at a time; (2) Top Disk Access: Only the topmost\n",
      "disk from any peg can be selected for movement; and (3) Size Ordering Constraint: A larger disk\n",
      "may never be placed on top of a smaller disk. This puzzle is a good evaluation testbed for reasoning\n",
      "and planning capabilities of models as it requires models to demonstrate key cognitive demands\n",
      "such as breaking down the problem into subproblems (recursive thinking), tracking multiple states\n",
      "and disk positions simultaneously (working memory management), adhering to movement rules and\n",
      "constraints while planning ahead (constraint satisfaction), and determining the correct order of\n",
      "operations to achieve the final goal (sequential planning).\n",
      "The minimum number of moves required to solve the Tower of Hanoi recursive puzzle with N disks\n",
      "is 2N −1, making it an exponentially scaling problem. This property allows for fine-grained difficulty\n",
      "control by adjusting the problem size with number of initial disks. However, in our evaluation\n",
      "framework, we focus on solution correctness rather than optimality, assessing each of the move’s\n",
      "validity and the model’s ability to reach the target state as the success criteria.\n",
      "16\n",
      "\n",
      "Prompt Design.\n",
      "The system prompt begins with a clear problem statement describing the puzzle\n",
      "setup. It explicitly states the movement rules and the objective of transferring all disks to the third\n",
      "peg. To facilitate understanding, the prompt includes example demonstrations as well as the critical\n",
      "formatting and reasoning expectations.\n",
      "System Prompt - Tower of Hanoi\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "There are three pegs and n disks of different sizes stacked on the first peg. The disks are\n",
      "numbered from 1 (smallest) to n (largest). Disk moves in this puzzle should follow:\n",
      "1. Only one disk can be moved at a time.\n",
      "2. Each move consists of taking the upper disk from one stack and placing it on top of\n",
      "another stack.\n",
      "3. A larger disk may not be placed on top of a smaller disk.\n",
      "The goal is to move the entire stack to the third peg.\n",
      "Example: With 3 disks numbered 1 (smallest), 2, and 3 (largest), the initial state is [[3, 2, 1],\n",
      "[], []], and a solution might be:\n",
      "moves = [[1, 0, 2], [2, 0, 1], [1, 2, 1], [3, 0, 2],\n",
      "[1, 1, 0], [2, 1, 2], [1, 0, 2]]\n",
      "This means: Move disk 1 from peg 0 to peg 2, then move disk 2 from peg 0 to peg 1, and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• The positions are 0-indexed (the leftmost peg is 0).\n",
      "• Ensure your final answer includes the complete list of moves in the format:\n",
      "moves = [[disk id, from peg, to peg], ...]\n",
      "The user prompt after the system prompt presents the specific puzzle instance with current configu-\n",
      "ration showing the distribution of disks across pegs and the goal configuration specifying the target\n",
      "state.\n",
      "User Prompt Template for $N$ Disks - Tower of Hanoi\n",
      "I have a puzzle with $N$ disks of different sizes with\n",
      "Initial configuration:\n",
      "• Peg 0: $N$ (bottom), . . . 2, 1 (top)\n",
      "• Peg 1: (empty)\n",
      "• Peg 2: (empty)\n",
      "17\n",
      "\n",
      "Goal configuration:\n",
      "• Peg 0: (empty)\n",
      "• Peg 1: (empty)\n",
      "• Peg 2: $N$ (bottom), . . . 2, 1 (top)\n",
      "Rules:\n",
      "• Only one disk can be moved at a time.\n",
      "• Only the top disk from any stack can be moved.\n",
      "• A larger disk may not be placed on top of a smaller disk.\n",
      "Find the sequence of moves to transform the initial configuration into the goal configuration.\n",
      "Simulator.\n",
      "Our evaluation framework employs separate puzzle simulators for each puzzle to\n",
      "ensure rigorous and consistent assessment of solutions obtained from LRMs. The Tower of Hanoi\n",
      "simulator is designed as a stateful environment that tracks disk configurations across three pegs\n",
      "and validates each proposed move against the puzzle’s fundamental constraints. The simulator\n",
      "architecture follows a modular design pattern with clear separation between state management,\n",
      "move validation, and solution verification. In this simulator, we have a puzzle class which tracks the\n",
      "current disk configuration and enforces the puzzle’s fundamental constraints. We also have a method\n",
      "to execute each move in the puzzle setup and perform four-layer validation: checking peg boundary\n",
      "conditions (0-2), verifying source pegs contain disks, confirming the specified disk is topmost, and\n",
      "enforcing the size ordering constraint that prevents larger disks from being placed on smaller ones.\n",
      "Upon successful validation, the method executes the disk transfer and updates the game state. Then,\n",
      "the complete solution validation is processed by sequentially processing move lists, and verifying\n",
      "goal state achievement.\n",
      "A.1.2\n",
      "Checker Jumping\n",
      "Problem Description.\n",
      "Checker Jumping is a one-dimensional constraint-satisfaction puzzle\n",
      "designed to test sequential reasoning, planning, and rule understanding capabilities. The puzzle\n",
      "consists of a linear arrangement of red checkers (’R’), blue checkers (’B’), and a single empty space\n",
      "(’_’). In the standard configuration, N red checkers are positioned on the left side, followed by an\n",
      "empty space in the middle, and N blue checkers on the right side, forming a linear board of length\n",
      "2N + 1. The objective is to swap the positions of all red and blue checkers, effectively mirroring the\n",
      "initial configuration, where red checkers end up on the right and blue checkers on the left. Movement\n",
      "in this puzzle is governed by two fundamental rules: (1) Slide Movement: A checker can slide\n",
      "forward into an adjacent empty space; and (2) Jump Movement: A checker can jump forward over\n",
      "exactly one checker of the opposite color to land in an empty space. Therefore, checkers cannot\n",
      "move backward toward their starting side—red checkers can only move rightward, and blue checkers\n",
      "can only move leftward from the initial configuration. This puzzle presents cognitive challenges that\n",
      "make it a great testbed for reasoning models. For example, models must demonstrate some aspect of\n",
      "spatial reasoning (tracking checker positions and possible moves), constraint satisfaction (adhering\n",
      "to movement rules during puzzle), lookahead planning (anticipating how current moves affect future\n",
      "18\n",
      "\n",
      "possibilities towards goal), and state-space exploration (searching through possible move sequences\n",
      "to find a valid solution path).\n",
      "The difficulty of the Checker Jumping puzzle scales with the number of checkers: with N checkers of\n",
      "each color, the minimum solution requires (N + 1)2 −1 moves, creating a quadratic relationship\n",
      "between problem size and solution complexity. In our evaluation framework, we mainly focus on\n",
      "solution correctness rather than optimality, evaluating each move against the puzzle constraints and\n",
      "confirming that the final state matches the goal configuration. This approach allows us to precisely\n",
      "identify reasoning failures and constraint violations that might occur during the solution process.\n",
      "Prompt Design.\n",
      "The system prompt begins with a clear problem statement describing the puzzle\n",
      "setup and movement rules. It explicitly states the objective and provides a concrete example with a\n",
      "small board configuration to illustrate how moves should be represented.\n",
      "System Prompt - Checker Jumping\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "On a one-dimensional board, there are red checkers (’R’), blue checkers (’B’), and one empty\n",
      "space (’_’). A checker can move by either:\n",
      "1. Sliding forward into an adjacent empty space, or\n",
      "2. Jumping over exactly one checker of the opposite color to land in an empty space.\n",
      "The goal is to swap the positions of all red and blue checkers, effectively mirroring the initial\n",
      "state.\n",
      "Example: If the initial state is [’R’, ’_’, ’B’], the goal is to reach [’B’, ’_’, ’R’]. Your solution\n",
      "should be a list of moves where each move is represented as [checker_color, position_from,\n",
      "position_to]. For example:\n",
      "moves = [[’R’, 0, 1], [’B’, 2, 0], [’R’, 1, 2]]\n",
      "This means: Move the red checker from position 0 to 1, then move the blue checker from\n",
      "position 2 to 0, and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• The positions are 0-indexed (the leftmost position is 0).\n",
      "• Ensure your final answer includes the complete list of moves for final solution in the\n",
      "format: moves = [[checker_color, position_from, position_to], ...]\n",
      "The user prompt presents the specific puzzle instance with the initial board configuration, and the\n",
      "goal state.\n",
      "19\n",
      "\n",
      "User Prompt Template for $N$ Checkers - Checker Jumping\n",
      "I have a puzzle with 2$N$+1 positions, where $N$ red checkers (’R’) on left, $N$ blue checkers\n",
      "(’B’) on right, and one empty space (’_’) in between are arranged in a line.\n",
      "Initial board: R R ... R _ B B ... B\n",
      "Goal board: B B ... B _ R R ... R\n",
      "Rules:\n",
      "• A checker can slide into an adjacent empty space.\n",
      "• A checker can jump over exactly one checker of the opposite color to land in an empty\n",
      "space.\n",
      "• Checkers cannot move backwards (towards their starting side).\n",
      "Find the minimum sequence of moves to transform the initial board into the goal board.\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating Checker Jumping\n",
      "puzzle solutions. The simulator implements a comprehensive validation system that enforces all\n",
      "puzzle constraints while tracking the state evolution throughout the solution path. The Checker\n",
      "Jumping simulator is designed as a stateful environment that tracks the position of all checkers and\n",
      "the empty space, validating each move of a given solution against the puzzle’s movement rules. The\n",
      "simulator begins by validating that both the initial and goal states are well-formed, containing the\n",
      "same number of red and blue checkers and exactly one empty space. Then, each move is executed\n",
      "with a method that performs multi-layer validation: verifying position boundaries, confirming correct\n",
      "checker color at source, ensuring target positions are empty, and validating move types as either\n",
      "slides (distance=1) or jumps (distance=2). The simulator enforces directional constraints preventing\n",
      "backward movement (red checkers move right, blue checkers move left) and validates jump moves\n",
      "by confirming the presence of an opposite-colored checker in the middle position. Upon successful\n",
      "validation, the method executes the checker transfer by updating positions and clearing the source.\n",
      "Then, the complete move sequences are processed with final goal state verification.\n",
      "A.1.3\n",
      "River Crossing\n",
      "Problem Description.\n",
      "River Crossing is a constraint satisfaction planning puzzle that tests multi-\n",
      "agent coordination and constraint management. This puzzle is a generalization of classic problems\n",
      "such as the Missionaries and Cannibals problem and the Bridge and Torch problem, which have been\n",
      "widely studied in planning literature [45, 46]. The river crossing puzzle involves N actors (denoted by\n",
      "a1, a2, ..., aN) and their corresponding N agents (denoted by A1, A2, ..., AN) who must cross a river us-\n",
      "ing a boat. In the initial state, all 2N individuals are on the left bank of the river. The goal is to trans-\n",
      "port everyone safely to the right bank. The puzzle operates under several key movement constraints:\n",
      "(1) Boat Capacity Constraint: The boat can carry at most k individuals at a time, where k is typically\n",
      "set to 2 for smaller puzzles (N ≤3) and 3 for larger puzzles (N ≤5); (2) Non-Empty Boat Constraint:\n",
      "The boat cannot travel empty and must have at least one person aboard; (3) Safety Constraint: An\n",
      "actor cannot be in the presence of another agent unless their own agent is also present, as agents must\n",
      "protect their clients from competing agents. This safety constraint applies both on the banks and in\n",
      "the boat. This puzzle requires complex planning and state tracking as participants must carefully coor-\n",
      "dinate their crossings while maintaining safety constraints at all times. The solver must reason through\n",
      "20\n",
      "\n",
      "different combinations of individuals who can safely travel together, determine who should return with\n",
      "the boat after a crossing, and strategically plan a sequence that eventually brings everyone to the right\n",
      "bank without violating any constraints. The complexity of this task can be controlled by adjusting the\n",
      "number of actor-agent pairs and the boat capacity, creating a scalable challenge for reasoning models.\n",
      "Prompt Design.\n",
      "The system prompt introduces the notation for representing actors and agents,\n",
      "establishes the solution format as a list of boat moves, and provides a simple example to demonstrate\n",
      "the format.\n",
      "System Prompt - River Crossing\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "You can represent actors with a_1, a_2, ... and agents with A_1, A_2, ... . Your solution\n",
      "must be a list of boat moves where each move indicates the people on the boat. For example, if\n",
      "there were two actors and two agents, you should return:\n",
      "moves =[[\" A_2\", \"a_2\"], [\"A_2\"], [\"A_1\", \"A_2\"], [\"A_1\"], [\"A_1\", \"a_1\n",
      "\"]]\n",
      "which indicates that in the first move, A_2 and a_2 row from left to right, and in the second\n",
      "move, A_2 rows from right to left and so on.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of boat moves.\n",
      "• The list shouldn’t have comments.\n",
      "• Ensure your final answer also includes the complete list of moves for final solution.\n",
      "The user prompt presents the specific puzzle instance with N actor-agent pairs, and the boat capacity\n",
      "k, and the safety constraint that must be maintained throughout the solution.\n",
      "User Prompt Template for $N$ Pairs - River Crossing\n",
      "$N$ actors and their $N$ agents want to cross a river in a boat that is capable of holding\n",
      "only $k$ people at a time, with the constraint that no actor can be in the presence\n",
      "of another agent, including while riding the boat, unless their own agent is also\n",
      "present, because each agent is worried their rivals will poach their client. Initially, all actors\n",
      "and agents are on the left side of the river with the boat. How should they cross the river?\n",
      "(Note: the boat cannot travel empty)\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating River Crossing\n",
      "puzzle extracted solutions. The simulator tracks the state of all individuals (actors and agents) and\n",
      "the boat position while enforcing all puzzle constraints. Each move is executed with multi-step\n",
      "validation: checking boat capacity limits, verifying all passengers are on the boat’s current side,\n",
      "and enforcing the critical safety constraint that actors cannot be in the presence of other agents\n",
      "without their own agent present, both on the boat and on each bank after the move. The simulator\n",
      "21\n",
      "\n",
      "manages dynamic boat positioning, automatically switching sides after each crossing, and validates\n",
      "the complete state after each move to ensure no safety violations occur on either bank. Then, the\n",
      "complete crossing sequences are verified that all 2N individuals successfully reach the right bank.\n",
      "A.1.4\n",
      "Blocks World\n",
      "Problem Description.\n",
      "Blocks World is a classical planning puzzle that has been recently studied\n",
      "for analyzing the planning capabilities of LLMs [37, 38]. The puzzle involves multiple stacks of blocks\n",
      "(A, B, C, etc.) that must be rearranged from an initial configuration to a specified goal configuration.\n",
      "Each block is uniquely identified by its letter, and the objective is to find the minimum sequence of\n",
      "moves needed to transform the initial state into the goal state. The puzzle operates only under two\n",
      "fundamental constraints: (1) Top Block Movement: Only the topmost block from any stack can be\n",
      "moved; and (2) Valid Placement: A block can only be placed either on an empty position or on top\n",
      "of another block. These constraints create planning problem where the order of operations becomes\n",
      "critical, as some configurations may require temporary placement of blocks to access those beneath\n",
      "them later. Blocks World serves as a great testbed for evaluating planning capabilities in reasoning\n",
      "models because it requires forward thinking, and state tracking. Recent studies have examined this\n",
      "puzzle in various configurations, including simplified settings with as few as 3 to 5 blocks, to evaluate\n",
      "LLM performance on sequential planning tasks [37, 38]. Models must demonstrate the ability to\n",
      "decompose complex state transformations into valid sequential moves, reason about dependencies\n",
      "between blocks (e.g., unblocking lower blocks before accessing them), and efficiently plan paths to\n",
      "the goal state without illegal moves.\n",
      "The difficulty of this puzzle can be scaled by adjusting several parameters: the number of blocks, the\n",
      "number of stacks, and the complexity of the initial and goal configurations. We primarily control\n",
      "complexity through the block count N, while following clear structural patterns in the initial and\n",
      "goal configurations. In our experimental design, the initial configuration consistently divides the\n",
      "N blocks between two stacks in alphabetical order, with the third stack empty as workspace. The\n",
      "goal configuration consolidates all blocks onto the first stack in a systematic interleaved pattern\n",
      "that alternates between blocks from the two initial stacks, with specific positioning that requires\n",
      "complete disassembly and reassembly of the existing stacks. For example, for N = 4, the initial\n",
      "state has blocks divided between two stacks [[\"A\", \"B\"], [\"C\", \"D\"], []] and the goal state\n",
      "[[\"D\", \"B\", \"C\", \"A\"], [], []] requires interleaving blocks from both stacks; and for N = 6,\n",
      "the initial state [[\"A\", \"B\", \"C\"], [\"D\", \"E\", \"F\"], []] must be transformed to [[\"F\", \"C\",\n",
      "\"E\", \"B\", \"D\", \"A\"], [], []], forming a complex alternating pattern. As N increases, the state\n",
      "space grows factorially, and the minimum solution length increases approximately linearly with\n",
      "N. For small values of N (2-7), the puzzles test basic planning; for medium values (8-20), they\n",
      "require more complex reasoning with longer planning horizons; and for large values (N > 20), they\n",
      "challenge the limits of sequential reasoning capabilities by requiring extensive temporary movements\n",
      "and pattern recognition across lengthy solution paths.\n",
      "Prompt Design.\n",
      "The system prompt introduces the fundamental rules of the Blocks World puzzle,\n",
      "establishes the move representation format, and provides a simple example to demonstrate the\n",
      "solution structure.\n",
      "22\n",
      "\n",
      "System Prompt - Blocks World\n",
      "You are a helpful assistant. Solve this puzzle for me.\n",
      "In this puzzle, there are stacks of blocks, and the goal is to rearrange them into a target\n",
      "configuration using a sequence of moves where:\n",
      "• Only the topmost block from any stack can be moved.\n",
      "• A block can be placed either on an empty position or on top of another block.\n",
      "Example:\n",
      "With initial state [[\"A\", \"B\"], [\"C\"], []] and goal state [[\"A\"], [\"B\"],\n",
      "[\"C\"]], a solution might be:\n",
      "moves = [[\"C\", 1, 2], [\"B\", 0, 1]]\n",
      "This means: Move block C from stack 1 to stack 2, then move block B from stack 0 to stack 1.\n",
      "Requirements:\n",
      "• When exploring potential solutions in your thinking process, always include the corre-\n",
      "sponding complete list of moves.\n",
      "• Ensure your final answer also includes the complete list of moves for final solution in the\n",
      "format: moves = [[block, from stack, to stack], ...]\n",
      "The user prompt presents the specific puzzle instance with the initial and goal configurations provided,\n",
      "and explicitly reminds the model about the movement constraint.\n",
      "User Prompt Template for $N$ Blocks - BlocksWorld\n",
      "I have a puzzle with $N$ blocks.\n",
      "Initial state:\n",
      "Stack 0: $blocks_0$ (top)\n",
      "Stack 1: $blocks_1$ (top)\n",
      "...\n",
      "Stack $m$: $blocks_m$ (top)\n",
      "Goal state:\n",
      "Stack 0: $goal_blocks_0$ (top)\n",
      "Stack 1: $goal_blocks_1$ (top)\n",
      "...\n",
      "Stack $m$: $goal_blocks_m$ (top)\n",
      "Find the minimum sequence of moves to transform the initial state into the goal state. Remember\n",
      "that only the topmost block of each stack can be moved.\n",
      "23\n",
      "\n",
      "Simulator.\n",
      "Our evaluation framework employs a custom simulator for validating Blocks World\n",
      "puzzle extracted solutions. The simulator manages the state of all blocks across stacks while enforcing\n",
      "the puzzle’s movement constraints. Each move is executed in the puzzle setup with three-layer\n",
      "validation: verifying stack indices are within bounds, confirming the source stack contains blocks,\n",
      "and ensuring the specified block is at the top of its stack (enforcing the top-block-only movement\n",
      "rule). Upon successful validation, the block transfer is executed and the block is popped from the\n",
      "source stack and appended to the destination stack. Finally, the complete solution sequences of block\n",
      "movements are processed and verified that the resulting configuration matches the target goal state.\n",
      "A.2\n",
      "Implementation Details\n",
      "Configurations\n",
      "Our experiments primarily utilized reasoning models and their non-thinking\n",
      "counterparts to enable thorough analysis of the thinking process. We specifically selected Claude\n",
      "3.7 Sonnet (thinking/non-thinking) and DeepSeek-R1/V3 due to their ability to provide access to\n",
      "thinking traces, a critical requirement for our analysis. For experiments focused solely on final\n",
      "accuracy metrics, we also included results from OpenAI’s o3-mini models, as they lack access to\n",
      "thoughts. For Claude 3.7 Sonnet (thinking and non-thinking) models we used maximum generation\n",
      "budget of 64,000 tokens, accessed through the API interface. Temperature is set to 1.0 for all API\n",
      "rus (Claude-3.7-Sonnet and o3-mini runs). The experiments with DeepSeek-R1, DeepSeek-V3, and\n",
      "DeepSeek-R1-Distill-Qern-32B are conducted on local servers with maximum generation length set to\n",
      "64,000 and temperature set to 1.0. In all experiments, we generated 25 samples per puzzle instance\n",
      "at each complexity level (N value) and reported performance averages across all samples.\n",
      "Solution Extraction\n",
      "A custom extraction pipeline was developed to process model responses\n",
      "and intermediate reasoning traces (thoughts). The pipeline consists of several key components. We\n",
      "implemented a flexible regex-based extractors to identify potential solution attempts in both the\n",
      "final response and thinking trace. The extraction process identify solution patterns using regular\n",
      "expressions (both explicit “moves =” patterns and alternative bracket-based solutions). We process\n",
      "and clean each extracted candidate solution by (i) Removing comments from the list (text following\n",
      "\"#\" in any line), and (ii) Normalizing move formats to what suggested in context to ensure consistent\n",
      "structure. Then, we validate solution format and structure to filter out invalid matches. During\n",
      "the extraction, we also capture metadata of token position for each extracted solution. Notably, for\n",
      "accurate position tracking within thinking traces, we employed the same tokenizer (cl100k_base)\n",
      "as the corresponding model to count tokens across all experiments. Token positions were also\n",
      "normalized with respect to thought length to enable cross-sample comparison. Finally, we make sure\n",
      "that the recorded solutions within the thought trace are unique and duplicate solutions (identical\n",
      "moves list) were filtered. In case of duplicate solutions, only the first solution is recorded for analysis.\n",
      "Solution Evaluation\n",
      "After extraction, each solution candidate is passed to the corresponding\n",
      "simulator of puzzle for fine-grained verification. The simulator takes a solution as list of moves and\n",
      "evaluate that with respect to the puzzle (check App. A.1 for details of each puzzle simulator). Each\n",
      "move in the compositional solution is executed sequentially according to previous moves and the\n",
      "puzzle rules. Then, the final state obtained from all moves in the sequence is compared to the goal\n",
      "state of puzzle to determine full solution correctness. For incorrect solutions, details of first failure\n",
      "move and the type of failure is also collected during the move verification with puzzle simulator.\n",
      "Execution of Prescribed Steps\n",
      "In addition to open-ended problem solving across different\n",
      "puzzles, we also conducted focused experiments to test how providing the explicit solving algorithm\n",
      "24\n",
      "\n",
      "guidance with prescribed steps would affect behavior of these reasoning models (Sec. 4.4).\n",
      "We expected that finding and devising solution from scratch should require substantially more\n",
      "computation for model (e.g., for search and verification) than just following a given algorithm’s\n",
      "steps. However, results in Figures 8a and 8b show that reasoning models’ behavior does not change\n",
      "that much and the collapse still occurs at roughly same points as before with this setting. This\n",
      "finding strengthens evidence that the limitation is not just in problem-solving and solution strategy\n",
      "discovery but also in consistent logical verification and step execution limitation throughout the\n",
      "generated reasoning chains.\n",
      "For example, models are provided with a complete recursive algorithm of solving Tower of Hanoi\n",
      "puzzle as follows. This algorithm scratchpad was appended to the standard problem prompt to test\n",
      "its impact on reasoning behavior.\n",
      "Example of Prescribed Algorithm for Tower of Hanoi\n",
      "Here is a pseudocode of recursive algorithm to solve the puzzle:\n",
      "ALGORITHM Solve(n, source, target, auxiliary, moves)\n",
      "// n = number of disks to move\n",
      "// source = starting peg (0, 1, or 2)\n",
      "// target = destination peg (0, 1, or 2)\n",
      "// auxiliary = the unused peg (0, 1, or 2)\n",
      "// moves = list to store the sequence of moves\n",
      "IF n equals 1 THEN\n",
      "// Get the top disk from source peg\n",
      "disk = the top disk on the source peg\n",
      "// Add the move to our list: [disk_id, source, target]\n",
      "ADD [disk, source, target] to moves\n",
      "RETURN\n",
      "END IF\n",
      "// Move n-1 disks from source to auxiliary peg\n",
      "Solve(n-1, source, auxiliary, target, moves)\n",
      "// Move the nth disk from source to target\n",
      "disk = the top disk on the source peg\n",
      "ADD [disk, source, target] to moves\n",
      "// Move n-1 disks from auxiliary to target\n",
      "Solve(n-1, auxiliary, target, source, moves)\n",
      "END ALGORITHM\n",
      "To solve the entire puzzle of moving n disks from peg 0 to peg 2:\n",
      "1. Initialize an empty list ’moves’\n",
      "2. Execute Solve(n, 0, 2, 1, moves)\n",
      "3. The ’moves’ list will contain the complete solution\n",
      "25\n",
      "\n",
      "Note: When executing this pseudocode, track which disk is currently on top of each peg. The\n",
      "disk IDs in the moves list should correspond to the actual disk being moved.\n",
      "You can use this algorithm as a scratchpad to help you solve the problem step by step.\n",
      "A.3\n",
      "Details on Computational Complexity\n",
      "A.3.1\n",
      "Compositional Depth Characterization\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Problem Size (N)\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Compositional Depth (# of Moves)\n",
      "Blocks World\n",
      "Checker Jumping\n",
      "River Crossing\n",
      "Tower of Hanoi\n",
      "Figure 9: Compositional depth (number of\n",
      "moves required) across different problem sizes\n",
      "for our four puzzle environments.\n",
      "Compositional depth is the number of sequential op-\n",
      "erations (i.e., moves) required to reach a full solution.\n",
      "Figure 9 demonstrates how this depth scales with\n",
      "problem size (N) across our four puzzle environments.\n",
      "Each puzzle has a distinct growth pattern, reflecting\n",
      "its underlying computational complexity. For exam-\n",
      "ple, Tower of Hanoi shows exponential growth (2N−1),\n",
      "and Checker Jumping displays quadratic scaling (\n",
      "(N + 1)2 −1). The River Crossing and Blocks World\n",
      "puzzles show more moderate, near-linear growth with\n",
      "N. These varying compositional depth profiles enable\n",
      "us to evaluate how language reasoning models handle\n",
      "different types of sequential reasoning challenges and\n",
      "if their accuracy is always correlated with the com-\n",
      "positional depth required to solve the puzzle. More\n",
      "details regarding this analysis is provided in Figure 10\n",
      "in App. A.4.\n",
      "A.3.2\n",
      "Performance vs Compositional Depth\n",
      "While intuition suggests a negative correlation between problem complexity and model accuracy, our\n",
      "analysis reveals a more nuanced relationship between compositional depth and LRM performance.\n",
      "Figure 10 demonstrates this across three state-of-the-art reasoning models (Claude-3.7-Sonnet w.\n",
      "thinking, DeepSeek-R1, and o3-mini) on our puzzle suite. Within individual puzzle types, we observe\n",
      "the expected negative correlation: as compositional depth increases, model accuracy consistently\n",
      "decreases. However, across different puzzle types, this relation breaks. Models may struggle with\n",
      "puzzles of lower compositional depth while succeeding on different puzzles with higher compositional\n",
      "depth. . For instance, models achieve >50% accuracy on Tower of Hanoi instances requiring\n",
      "approximately 102 moves, yet consistently fail on River Crossing puzzles with substantially lower\n",
      "compositional depth (∼101 moves).\n",
      "A.4\n",
      "Extended Results and Analysis\n",
      "Failure Analysis.\n",
      "Understanding where models fail within the compositional reasoning steps\n",
      "provides insights beyond binary success metrics. Our accuracy evaluation requires perfect execution\n",
      "of entire move sequences—a single incorrect move results in failure. To examine failure patterns\n",
      "more granularly, we analyze the compositional depth at which models first make incorrect moves\n",
      "across varying problem complexity levels.\n",
      "26\n",
      "\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "DeepSeek-R1\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "Claude-3.7-Sonnet (thinking)\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Compositional Depth (# of Moves)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy (%)\n",
      "o3-mini (high)\n",
      "Tower Hanoi\n",
      "Checker Jumping\n",
      "River Crossing\n",
      "Blocks World\n",
      "Figure 10: Accuracy versus compositional depth (number of moves required) for three LRMs\n",
      "(DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across four puzzle environments.\n",
      "Figure 11 shows the failure move ID versus problem complexity (N) within the solution sequence.\n",
      "The top row compares Claude-3.7-Sonnet with and without thinking capabilities, while the bottom\n",
      "row compares DeepSeek-R1 (thinking) with DeepSeek-V3 (non-thinking).\n",
      "These comparisons\n",
      "demonstrates how thinking mechanisms of LRMs influence failure patterns in compositional reasoning\n",
      "tasks of puzzles. Several counterintuitive patterns emerge from our analysis. First, models exhibit\n",
      "non-monotonic failure behavior with respect to problem complexity—instances where models fail\n",
      "earlier in the solution sequence for higher N values despite requiring longer overall solutions. For\n",
      "example, in Tower of Hanoi, models sometimes fail at below 50 moves for N = 15 but succeed through\n",
      "more than 100 moves for N = 8, contradicting the expectation that effective algorithmic planning\n",
      "and execution for the same puzzle should maintain consistent failure patterns relative to solution\n",
      "progress. This suggests fundamental inconsistencies in how models (both LRMs and their non-\n",
      "thinking standard LLM counterparts) apply learned solution strategies across different problem scales.\n",
      "Also, we observe that in the high-complexity regimes where both model variants experience complete\n",
      "accuracy collapse, e.g., Tower of Hanoi with N ≥15 and Blocks World with N ≥40, non-thinking\n",
      "models occasionally sustain performance deeper into the solution sequence and are able to fail at later\n",
      "moves than thinking-enabled variants. This is interesting as it shows that compositional reasoning\n",
      "failures in LLMs are not simply due to insufficient context length or inference compute, but rather\n",
      "reflect fundamental limitations in how models maintain algorithmic consistency across problem scales.\n",
      "We also analyze the distributional characteristics of failure moves to understand the consistency and\n",
      "reliability of model reasoning. Figure 12 presents the density distributions of failure move positions\n",
      "aggregated across all problem complexities for each puzzle environment, comparing thinking and\n",
      "non-thinking models within the same family. Based on the figure, thinking models (Claude-3.7-Sonnet\n",
      "with thinking and DeepSeek-R1) consistently show higher mean failure positions across all puzzles,\n",
      "as indicated by the dashed vertical lines showing mean of first failure in sequence of moves. However,\n",
      "the distribution shape of thinking models mostly have higher variance in their failure patterns. This\n",
      "suggests that while these models can reach deeper into solution sequences on average, their reasoning\n",
      "processes are more instable and prone to inconsistent performance.\n",
      "Reasoning Effort Dynamics.\n",
      "Figure 13 demonstrates the reasoning effort (measured by inference\n",
      "thinking tokens) versus problem complexity across our puzzle environments. Green dots indicate\n",
      "27\n",
      "\n",
      "Figure 11: The first failure move versus problem complexity (N) comparison for thinking and\n",
      "non-thinking models across puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom:\n",
      "DeepSeek-R1 vs DeepSeek-V3.\n",
      "correct solutions, red crosses show incorrect ones, and blue lines track average thinking token usage at\n",
      "each complexity level (N) across different puzzles and LRMs. We observe a consistent pattern across\n",
      "all three reasoning models (DeepSeek-R1, Claude-3.7-Sonnet-thinking, o3-mini) where thinking token\n",
      "usage, i.e. reasoning effort, initially scales with problem complexity but counterintuitively declines\n",
      "after reaching a model-specific threshold. This suggests an interesting and fundamental scaling limit\n",
      "in LRM thinking process for reasoning where beyond certain complexity thresholds, models not\n",
      "only fail to solve problems but counterintuitively reduce their inference compute despite facing more\n",
      "difficult problems and being well below the context and generation limits.\n",
      "28\n",
      "\n",
      "Figure 12: Density distribution of first failure moves for thinking and non-thinking models across\n",
      "puzzle environments. Top: Claude-3.7-Sonnet comparison; Bottom: DeepSeek-R1 vs DeepSeek-V3.\n",
      "29\n",
      "\n",
      "Figure 13: Detailed results on reasoning effort (measured in inference thinking tokens) versus problem\n",
      "complexity (N) for three LRMs (DeepSeek-R1, Claude-3.7-Sonnet with thinking, and o3-mini) across\n",
      "four puzzle environments.\n",
      "30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Who are the authors?\"\n",
    "query_embedding = model.encode(\"query: \" + query)\n",
    "query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "context = chunks[I[0][0]]\n",
    "\n",
    "print(\"Most Relevant Context:\\n\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f9344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
